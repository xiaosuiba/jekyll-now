[
    
        
            {
                "ref": "https://xiaosuiba.github.io/2021/05/10/2021-05-10-how-does-tkestack-localidentity-make-use-of-generatename/",
                "title": "TKEStack中LocalIdentity如何利用generateName机制生成Name",
                "section": "post",
                "date" : "2021.05.10",
                "body": "本文从发现TKEStack自动生成localidentity名称机制出发，稍微探索了kubernetes generateName的作用机理。\n日前在研究TKEStack用户机制中，发现创建LocalIdentiy时，并没有传入metadata.name，而最终系统会生成一个随机值，类似：\nusr-2t455l2z\rusr-dsa54sdf\rusr-xxxxxxxx\r这显然是一个随机值。由于我们希望自定义这个值，所以进行了一番测试，结果发现：\n 不设置metadata.name时，生成随机值 设置metadata.name之后，使用设置值  显然这个值是可以直接设置的。但是我对这个随机值的生成产生了兴趣，带着问题翻看了下源码，在pkg\\auth\\registry\\localidentity\\strategy.go中，找到如下片段：\nfunc (Strategy) PrepareForCreate(ctx context.Context, obj runtime.Object) { localIdentity, _ := obj.(*auth.LocalIdentity) _, tenantID := authentication.UsernameAndTenantID(ctx) if len(tenantID) != 0 { localIdentity.Spec.TenantID = tenantID } if localIdentity.Name == \u0026#34;\u0026#34; \u0026amp;\u0026amp; localIdentity.GenerateName == \u0026#34;\u0026#34; { localIdentity.GenerateName = \u0026#34;usr-\u0026#34; } localIdentity.Spec.Finalizers = []auth.FinalizerName{ auth.LocalIdentityFinalize, } } 这是localIdentity reststorage的创建预处理函数，可以看到其中对localIdentity.Name做了检查，但是这里却没有直接生成随机值，而是对GenerateName进行了一个赋值。那最终的随机值Name是怎么生成了呢？\n看到这个usr-，其实已经猜到了一大半，猜想这里设置的是一个前缀，而随机值部分应该是由K8s自身的机制实现的。官网相关文档解释如下：\n Generated values Some values of an object are typically generated before the object is persisted. It is important not to rely upon the values of these fields set by a dry-run request, since these values will likely be different in dry-run mode from when the real request is made. Some of these fields are:\n name: if generateName is set, name will have a unique random name creationTimestamp/deletionTimestamp: records the time of creation/deletion UID: uniquely identifies the object and is randomly generated (non-deterministic) resourceVersion: tracks the persisted version of the object Any field set by a mutating admission controller For the Service resource: Ports or IPs that kube-apiserver assigns to v1.Service objects   这里说得比较含糊，而stackoverflow中有个问题说得很明确：\n You can replace name with generateName, which adds a random suffix.\n 以后在需要随机值的场合，可以不用自己动手了，直接依赖系统机制即可。\n"
            }
        
    ,
        
            {
                "ref": "https://xiaosuiba.github.io/2021/04/13/2021-04-13-tkestack-monitor-controller-issue/",
                "title": "TKEStack v1.6.0 1.19版本中pod告警策略失效问题分析",
                "section": "post",
                "date" : "2021.04.13",
                "body": "本文从探索了TKEStack 1.6中部分告警失效的原因，并提出修复手段。\n日前在使用TKEStack v1.6.0的时候，发现针对工作负载的（也就是pod状态、重启次数）告警失效。我们进行了一定的分析，最终找到原因和Kubernetes 1.19以后版本中一个kubelet metrics名称修改有关。\nTKEStack v1.6.0针对pod的告警使用了k8s_pod_status_ready, k8s_pod_restart_total两个metrics作为判定参数。继续查看这两个metrics的定义，发现他们都与一个名为__pod_info2的metrics相关，其定义如下：\n- record: __pod_info2 expr: label_replace(label_replace(__pod_info1{workload_kind=\u0026#34;ReplicaSet\u0026#34;} * on (workload_name,namespace) group_left(owner_name, owner_kind) label_replace(kube_replicaset_owner,\u0026#34;workload_name\u0026#34;,\u0026#34;$1\u0026#34;,\u0026#34;replicaset\u0026#34;,\u0026#34;(.*)\u0026#34;),\u0026#34;workload_name\u0026#34;,\u0026#34;$1\u0026#34;,\u0026#34;owner_name\u0026#34;,\u0026#34;(.*)\u0026#34;),\u0026#34;workload_kind\u0026#34;,\u0026#34;$1\u0026#34;,\u0026#34;owner_kind\u0026#34;,\u0026#34;(.*)\u0026#34;) or on(pod_name,namesapce) __pod_info1{workload_kind != \u0026#34;ReplicaSet\u0026#34;} __pod_info2又与__pod_info1相关，其定义如下：\n- record: __pod_info1 expr: kube_pod_info* on(node) group_left(node_role) kube_node_labels 由__pod_info1，关联到kube_node_labels。到这里时，我们发现在v1.19版本中，kube_node_labels已经没有数据，看来问题就出在这里。查看其定义如下：\n- record: kube_node_labels expr: kubelet_running_pod_count*0 + 1 定义很简单，只与kubelet_running_pod_count相关，而这个值在prometheus中也未能查询到，看来问题发生在这里。\n简单的google之后，我们迅速发现了问题的原因：kubernetes在v1.19版本中进行了两个kubelet metrics的修改(kubernetes/kubernetes#92407)：\nkubelet: following metrics have been renamed:\rkubelet_running_container_count --\u0026gt; kubelet_running_containers\rkubelet_running_pod_count --\u0026gt; kubelet_running_pods\r该修改造成了kube_node_labels的失效，从而导致许多tke自定义的metrics失效。\n解决方案有两种：\n 按选择版本配置不同的prometheus 使用正则表达式匹配所有版本的metrics  毫无疑问第二种方式更加简单。相关代码位于：pkg/monitor/controller/prometheus/yamls.go 测试之后问题解决。相关issue和pr已经提交到TKEStack。\n"
            }
        
    ,
        
            {
                "ref": "https://xiaosuiba.github.io/2021/04/06/2021-04-06-tkestack-auth-api-issue/",
                "title": "TKEStack v1.6.0 global集群中serviceaccount总是默认拥有所有权限",
                "section": "post",
                "date" : "2021.04.06",
                "body": "本文从探索了TKEStack 1.6中global集群任何serviceaccount均具有cluster-admin权限原因。\nTKEStack v1.6.0已经发布了，没有包含重大更新，但是在使用过程中，我们发现了一个很神奇的现象：global集群中任何serviceaccount都能访问所有的集群资源。这点可以直接使用kubectl auth can-i得到验证：\n在1.5.0集群中执行：\n$ kubectl auth can-i create pod --as=system:serviceaccount:tke:fake no 而在1.6.0中执行：\n$ kubectl auth can-i create pod --as=system:serviceaccount:tke:fake yes 我们甚至都还没有创建过fake这个serviceaccount。根据现象，首先怀疑是kube-apiserver的authz配置发生了改变，查看kube-apiserver的配置，果然:\n--authorization-mode=Node,RBAC,Webhook\r1.5.0中则只有Node,RBAC，看来问题就出在Webhook中。关于Webhook的说明，官方文档给出了解释。继续查看kubernetes的Webhook配置，配置文件为--authorization-webhook-config-file=/etc/kubernetes/tke-authz-webhook.yaml，文件内容如下：\ntke-authz-webhook.yaml: |apiVersion: v1 kind: Config clusters: - name: tke cluster: certificate-authority: /app/certs/ca.crt server: http://{vip}:31138/auth/authz users: - name: admin-cert user: client-certificate: /app/certs/admin.crt client-key: /app/certs/admin.key current-context: tke contexts: - context: cluster: tke user: admin-cert name: tke webhook代理地址为http://{vip}:31138/auth/authz，也即是tke-auth-api的nodeport。\n继续查看tke-auth-api中认证相关配置： /cmd/tke-auth-api/app/app.go\ncfg, err := config.CreateConfigFromOptions(basename, opts) tke-auth-api/app/config/config.go\naggregateAuthz, err := aggregation.NewAuthorizer(authClient, opts.Authorization, opts.Auth, enforcer, opts.Authentication.PrivilegedUsername) /opt/project/tke/pkg/auth/authorization/aggregation/aggregation.go\n// NewAuthorizer creates a authorizer for subject access review and returns it. func NewAuthorizer(authClient authinternalclient.AuthInterface, authorizationOpts *options.AuthorizationOptions, authOpts *options.AuthOptions, enforcer *casbin.SyncedEnforcer, privilegedUsername string) (authorizer.Authorizer, error) { var ( authorizers []authorizer.Authorizer ) if len(authorizationOpts.WebhookConfigFile) != 0 { webhookAuthorizer, err := webhook.New(authorizationOpts.WebhookConfigFile, authorizationOpts.WebhookVersion, authorizationOpts.WebhookCacheAuthorizedTTL, authorizationOpts.WebhookCacheUnauthorizedTTL, nil) if err != nil { return nil, err } authorizers = append(authorizers, webhookAuthorizer) } if len(authorizationOpts.PolicyFile) != 0 { abacAuthorizer, err := abac.NewABACAuthorizer(authorizationOpts.PolicyFile) if err != nil { return nil, err } authorizers = append(authorizers, abacAuthorizer) } authorizers = append(authorizers, local.NewAuthorizer(authClient, enforcer, privilegedUsername)) return union.New(authorizers...), nil } 可以看到最终的authrizer配置由webhook（如果有）、abac（如果有）和local组成。auth-api的配置文件：\ntke-auth-api.toml: | ........ [authorization] policy_file=\u0026#34;/app/conf/abac-policy.json\u0026#34; 继续查看abac-policy.json\n{\u0026#34;apiVersion\u0026#34;:\u0026#34;abac.authorization.kubernetes.io/v1beta1\u0026#34;,\u0026#34;kind\u0026#34;:\u0026#34;Policy\u0026#34;,\u0026#34;spec\u0026#34;:{\u0026#34;user\u0026#34;:\u0026#34;system:*\u0026#34;,\u0026#34;namespace\u0026#34;:\u0026#34;*\u0026#34;, \u0026#34;resource\u0026#34;:\u0026#34;*\u0026#34;,\u0026#34;apiGroup\u0026#34;:\u0026#34;*\u0026#34;, \u0026#34;group\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;nonResourcePath\u0026#34;:\u0026#34;*\u0026#34;}} 该文件将配置任意system:*配置拥有任意namespace下的所有资源。\n至此，问题原因已经找到了。但问什么TKEStack中如此配置ABAC？这将导致一个明显的漏洞出现。带着问题，我们继续查看github上的修改提交记录：\n 1155 但是没有找到任何相关说明为何要如此修改，我将对此issue进行一个comment，希望作者能有相关解释。 "
            }
        
    ,
        
            {
                "ref": "https://xiaosuiba.github.io/2021/03/31/2021-03-31-tkestack-registry-dns-issue/",
                "title": "TKEStack组件不能访问registry域名问题",
                "section": "post",
                "date" : "2021.03.31",
                "body": "本文从探索了TKEStack 1.5版本中TKEStack组件不能访问registry域名的原因，并给出解决办法。\nTKEStack需要配置registry域名，默认为default.registry.tke.com，application组件将使用该域名解析到tke-registry-api服务（或者直接解析到gateway也可以）。\n在v1.5版本中，Coredns默认没有配置对该域名的处理，导致用户只能通过Coredns的forward插件，使用外部的DNS做处理；或者修改Coredns配置文件。\n很高兴看到在v1.6.0版本中，该问题得到了解决，Corefile中添加了如下配置：\n ………………\rrewrite name default.registry.tke.com tke-registry-api.tke.svc.cluster.local\r官方对于rewrite插件的解释如下：\n Rewrites are invisible to the client. There are simple rewrites (fast) and complex rewrites (slower), but they’re powerful enough to accommodate most dynamic back-end applications.\n 也即将default.registry.tke.comrewrite到tke-registry-api.tke.svc.cluster.local，也即tke-registry-api地址。\n相关pr: feat(registry): add registry\u0026rsquo;s domain names to coredns by tenant\n"
            }
        
    ,
        
            {
                "ref": "https://xiaosuiba.github.io/2021/03/29/2021-03-29-why-only-new-in-rest-storage-interface-copy/",
                "title": "Why rest.Storage interface contains only one method",
                "section": "post",
                "date" : "2021.03.29",
                "body": "rest.Storage interface contains only one method. This page show how could it utilize golang reflection to do the job.\nWhy rest.Storage interface have only one method While reading Kubernetes source code, I found one question really bothered me for several days. The common interface of rest storage is defined in /vendor/k8s.io/apiserver/pkg/registry/rest/rest.go:\ntype Storage interface { // New returns an empty object that can be used with Create and Update after request data has been put into it. \t// This object must be a pointer type for use with Codec.DecodeInto([]byte, runtime.Object) \tNew() runtime.Object } You can see there\u0026rsquo;s only one method New defined here. So how could the other actions, like Get or List etc., be completed?\nAfter several day\u0026rsquo;s search on the internet, I finally found the some code in /vendor/k8s.io/apiserver/pkg/endpoints/installer.go:\nfunc (a *APIInstaller) registerResourceHandlers(path string, storage rest.Storage, ws *restful.WebService) (*metav1.APIResource, *storageversion.ResourceInfo, error) { ..... // what verbs are supported by the storage, used to know what verbs we support per path \tcreater, isCreater := storage.(rest.Creater) namedCreater, isNamedCreater := storage.(rest.NamedCreater) lister, isLister := storage.(rest.Lister) getter, isGetter := storage.(rest.Getter) getterWithOptions, isGetterWithOptions := storage.(rest.GetterWithOptions) gracefulDeleter, isGracefulDeleter := storage.(rest.GracefulDeleter) collectionDeleter, isCollectionDeleter := storage.(rest.CollectionDeleter) updater, isUpdater := storage.(rest.Updater) patcher, isPatcher := storage.(rest.Patcher) watcher, isWatcher := storage.(rest.Watcher) connecter, isConnecter := storage.(rest.Connecter) storageMeta, isMetadata := storage.(rest.StorageMetadata) storageVersionProvider, isStorageVersionProvider := storage.(rest.StorageVersionProvider) see here, registerResourceHandlers utilizes golang type conversion to check the storage\u0026rsquo;s abilities. So the New method is only a sign of rest.storage interface. The real abilities could be defined in resource storage struct as needed. The methods defined in each specific resource storage type represents its ability in the same time.\n"
            }
        
    ,
        
            {
                "ref": "https://xiaosuiba.github.io/2021/03/26/2021-03-26-kubernetes-sample-apiserver/",
                "title": "Kubernetes sample-apiserver 代码阅读",
                "section": "post",
                "date" : "2021.03.26",
                "body": "启动过程 main.go:\nfunc main() { logs.InitLogs() defer logs.FlushLogs() stopCh := genericapiserver.SetupSignalHandler() options := server.NewWardleServerOptions(os.Stdout, os.Stderr) cmd := server.NewCommandStartWardleServer(options, stopCh) cmd.Flags().AddGoFlagSet(flag.CommandLine) if err := cmd.Execute(); err != nil { klog.Fatal(err) } } options 调用 server.NewWardleServerOption 构建了一个 WardleServerOptions 配置对象\ntype WardleServerOptions struct { RecommendedOptions *genericoptions.RecommendedOptions SharedInformerFactory informers.SharedInformerFactory StdOut io.Writer StdErr io.Writer } RecommendedOptions 的解释为：\n // RecommendedOptions contains the recommended options for running an API server.\n// If you add something to this list, it should be in a logical grouping.\n// Each of them can be nil to leave the feature unconfigured on ApplyTo.\n server.NewCommandStartWardleServer 中构建了一个 cobra.Command，代码很简短：\nfunc NewCommandStartWardleServer(defaults *WardleServerOptions, stopCh \u0026lt;-chan struct{}) *cobra.Command { o := *defaults cmd := \u0026amp;cobra.Command{ Short: \u0026#34;Launch a wardle API server\u0026#34;, Long: \u0026#34;Launch a wardle API server\u0026#34;, RunE: func(c *cobra.Command, args []string) error { if err := o.Complete(); err != nil { return err } if err := o.Validate(args); err != nil { return err } if err := o.RunWardleServer(stopCh); err != nil { return err } return nil }, } flags := cmd.Flags() o.RecommendedOptions.AddFlags(flags) utilfeature.DefaultMutableFeatureGate.AddFlag(flags) return cmd } 可以看到主要工作包括两部分，一个是使用flag填充o.RecommendedOption，这部分在调用时即完成；第而部分是配置cmd的启动RunE，主要有三个步骤：\n o.Complete(): 完成一些额外配置，如admission plugins o.Validate(args)：对配置进行校验，这里直接使用了默认校验（sample-apiserver没有额外的配置项） o.RunWardleServer(stopCh)：启动服务  下面重点看下o.RunWardleServer，这是启动服务的核心部分：\n// RunWardleServer starts a new WardleServer given WardleServerOptions func (o WardleServerOptions) RunWardleServer(stopCh \u0026lt;-chan struct{}) error { //从RecommendedOption产生最终config \tconfig, err := o.Config() if err != nil { return err } //对config进行补完并使用配置产生server \tserver, err := config.Complete().New() if err != nil { return err } server.GenericAPIServer.AddPostStartHookOrDie(\u0026#34;start-sample-server-informers\u0026#34;, func(context genericapiserver.PostStartHookContext) error { config.GenericConfig.SharedInformerFactory.Start(context.StopCh) o.SharedInformerFactory.Start(context.StopCh) return nil }) //启动服务 \treturn server.GenericAPIServer.PrepareRun().Run(stopCh) } o.Config()：\n// Config returns config for the api server given WardleServerOptions func (o *WardleServerOptions) Config() (*apiserver.Config, error) { …… if err := o.RecommendedOptions.ApplyTo(serverConfig); err != nil { return nil, err } config := \u0026amp;apiserver.Config{ GenericConfig: serverConfig, ExtraConfig: apiserver.ExtraConfig{}, } return config, nil } 从这里可以发现通常的apiserver.Config构建方式是通过RecommendedOptions与cobra.command协作构建出RecommendedOptions，然后使用RecommendedOptions.ApplyTo方法将配置填充到apiserver.Config中，用于启动最终的apiserver。\nserver, err := config.Complete().New()产生了最终的WardleServer：\nfunc (c completedConfig) New() (*WardleServer, error) { genericServer, err := c.GenericConfig.New(\u0026#34;sample-apiserver\u0026#34;, genericapiserver.NewEmptyDelegate()) if err != nil { return nil, err } s := \u0026amp;WardleServer{ GenericAPIServer: genericServer, } apiGroupInfo := genericapiserver.NewDefaultAPIGroupInfo(wardle.GroupName, Scheme, metav1.ParameterCodec, Codecs) v1alpha1storage := map[string]rest.Storage{} v1alpha1storage[\u0026#34;flunders\u0026#34;] = wardleregistry.RESTInPeace(flunderstorage.NewREST(Scheme, c.GenericConfig.RESTOptionsGetter)) v1alpha1storage[\u0026#34;fischers\u0026#34;] = wardleregistry.RESTInPeace(fischerstorage.NewREST(Scheme, c.GenericConfig.RESTOptionsGetter)) apiGroupInfo.VersionedResourcesStorageMap[\u0026#34;v1alpha1\u0026#34;] = v1alpha1storage v1beta1storage := map[string]rest.Storage{} v1beta1storage[\u0026#34;flunders\u0026#34;] = wardleregistry.RESTInPeace(flunderstorage.NewREST(Scheme, c.GenericConfig.RESTOptionsGetter)) apiGroupInfo.VersionedResourcesStorageMap[\u0026#34;v1beta1\u0026#34;] = v1beta1storage if err := s.GenericAPIServer.InstallAPIGroup(\u0026amp;apiGroupInfo); err != nil { return nil, err } return s, nil } New做了两个重要工作：\n 使用配置构建了genericServer 为apiGroup配置了restStorage  我们挑选flunderstorage.NewREST来看看如何构建一个默认的RESTStorage:\n// NewREST returns a RESTStorage object that will work against API services. func NewREST(scheme *runtime.Scheme, optsGetter generic.RESTOptionsGetter) (*registry.REST, error) { strategy := NewStrategy(scheme) store := \u0026amp;genericregistry.Store{ NewFunc: func() runtime.Object { return \u0026amp;wardle.Flunder{} }, NewListFunc: func() runtime.Object { return \u0026amp;wardle.FlunderList{} }, PredicateFunc: MatchFlunder, DefaultQualifiedResource: wardle.Resource(\u0026#34;flunders\u0026#34;), CreateStrategy: strategy, UpdateStrategy: strategy, DeleteStrategy: strategy, // TODO: define table converter that exposes more than name/creation timestamp \tTableConvertor: rest.NewDefaultTableConvertor(wardle.Resource(\u0026#34;flunders\u0026#34;)), } options := \u0026amp;generic.StoreOptions{RESTOptions: optsGetter, AttrFunc: GetAttrs} if err := store.CompleteWithOptions(options); err != nil { return nil, err } return \u0026amp;registry.REST{store}, nil } 这里直接使用genericregistry.Store构建了RESTStorage，按照RESTStorage-\u0026gt;RegistryStore-\u0026gt;Storage.Interface的流程，我们再深入一下genericregistry.Store，可以看到，在函数中我们并没有配置任何的Storage.Interface（genericregistry.Store中的Storage属性），在store.CompleteWithOptions(options)中：\nif e.Storage.Storage == nil { e.Storage.Codec = opts.StorageConfig.Codec var err error e.Storage.Storage, e.DestroyFunc, err = opts.Decorator( opts.StorageConfig, prefix, keyFunc, e.NewFunc, e.NewListFunc, attrFunc, options.TriggerFunc, options.Indexers, ) 当Storage为nil时，使用Decorator模式构建了一个e.Storage.Storage。\n此外，在apiserver.go中还使用init函数，完成了schema的注册工作：\nfunc init() { install.Install(Scheme) // we need to add the options to empty v1 \t// TODO fix the server code to avoid this \tmetav1.AddToGroupVersion(Scheme, schema.GroupVersion{Version: \u0026#34;v1\u0026#34;}) // TODO: keep the generic API server from wanting this \tunversioned := schema.GroupVersion{Group: \u0026#34;\u0026#34;, Version: \u0026#34;v1\u0026#34;} Scheme.AddUnversionedTypes(unversioned, \u0026amp;metav1.Status{}, \u0026amp;metav1.APIVersions{}, \u0026amp;metav1.APIGroupList{}, \u0026amp;metav1.APIGroup{}, \u0026amp;metav1.APIResourceList{}, ) } 到此为止，apiserver的配置已经完成，接下来我们看看服务的启动: server.GenericAPIServer.PrepareRun().Run(stopCh)\n// PrepareRun does post API installation setup steps. It calls recursively the same function of the delegates. func (s *GenericAPIServer) PrepareRun() preparedGenericAPIServer { s.delegationTarget.PrepareRun() if s.openAPIConfig != nil { s.OpenAPIVersionedService, s.StaticOpenAPISpec = routes.OpenAPI{ Config: s.openAPIConfig, }.Install(s.Handler.GoRestfulContainer, s.Handler.NonGoRestfulMux) } s.installHealthz() s.installLivez() err := s.addReadyzShutdownCheck(s.readinessStopCh) if err != nil { klog.Errorf(\u0026#34;Failed to install readyz shutdown check %s\u0026#34;, err) } s.installReadyz() // Register audit backend preShutdownHook. \tif s.AuditBackend != nil { err := s.AddPreShutdownHook(\u0026#34;audit-backend\u0026#34;, func() error { s.AuditBackend.Shutdown() return nil }) if err != nil { klog.Errorf(\u0026#34;Failed to add pre-shutdown hook for audit-backend %s\u0026#34;, err) } } return preparedGenericAPIServer{s} } 可以看到主要工作就是配置openAPI handler，安装Healthz、Livez、Readyz端点，接下来运行Run：\n// Run spawns the secure http server. It only returns if stopCh is closed // or the secure port cannot be listened on initially. func (s preparedGenericAPIServer) Run(stopCh \u0026lt;-chan struct{}) error { ............... // close socket after delayed stopCh \tstoppedCh, err := s.NonBlockingRun(delayedStopCh) ............... } func (s preparedGenericAPIServer) NonBlockingRun(stopCh \u0026lt;-chan struct{}) (\u0026lt;-chan struct{}, error) { ................ // Use an internal stop channel to allow cleanup of the listeners on error. \tinternalStopCh := make(chan struct{}) var stoppedCh \u0026lt;-chan struct{} if s.SecureServingInfo != nil \u0026amp;\u0026amp; s.Handler != nil { var err error stoppedCh, err = s.SecureServingInfo.Serve(s.Handler, s.ShutdownTimeout, internalStopCh) if err != nil { close(internalStopCh) close(auditStopCh) return nil, err } } ................ } 至此，APIServer启动完成。\n"
            }
        
    ,
        
            {
                "ref": "https://xiaosuiba.github.io/2019/02/02/2020-02-02-happy-spring-festival/",
                "title": "Happy Spring Festival",
                "section": "post",
                "date" : "2019.02.02",
                "body": "Happy Spring Festival 赶在春节前把博客搬迁成 hugo 了 :) 祝大家春节快乐，来年大吉，Happy Spring Festival~\n"
            }
        
    
]