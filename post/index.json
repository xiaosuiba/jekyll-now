[
    
        
            {
                "ref": "https://xiaosuiba.github.io/2021/06/29/how-to-custom-kubeadm-to-extend-certs-duration/",
                "title": "延长kubeadm集群证书有效期",
                "section": "post",
                "date" : "2021.06.29",
                "body": "使用kubeadm创建集群时，服务证书和kubelet均会在1年过期。本文介绍了一种修改kubeadm来延长各种证书有效期的方式。\n延长kubeadm集群证书有效期 0. 默认证书有效期 最近公司的一个产品出了线上问题，由于使用了默认的kubeadm创建集群，集群的CA证书有10年有效期，但其余证书（包括服务证书和kubelet证书）通常只有1年有效期，到期之后需要通过kubeadm升级，或者手动执行kubeadm renew的方式来续签证书。否则集群将有停止服务的风险。\n官方如此设计kubeadm，一是出于安全原因，另外是希望用户能够及时升级。每次使用kubeadm升级时，会自动更新证书有效期。但作为服务提供商，我们不能控制或强制用户升级，直接给100年有效期是最稳妥的做法。\n使用kubeadm certs check-expiration(1.20以前需要使用kubeadm alpha certs check-expiration)查看证书如下：\n[root@localhost kubernetes]# kubeadm certs check-expiration [check-expiration] Reading configuration from the cluster... [check-expiration] FYI: You can look at this config file with \u0026#39;kubectl -n kube-system get cm kubeadm-config -o yaml\u0026#39; CERTIFICATE EXPIRES RESIDUAL TIME CERTIFICATE AUTHORITY EXTERNALLY MANAGED admin.conf Jun 09, 2022 09:36 UTC 345d no apiserver Jun 09, 2022 09:36 UTC 345d ca no apiserver-etcd-client Jun 09, 2022 09:36 UTC 345d etcd-ca no apiserver-kubelet-client Jun 09, 2022 09:36 UTC 345d ca no controller-manager.conf Jun 09, 2022 09:36 UTC 345d no etcd-healthcheck-client Jun 09, 2022 09:36 UTC 345d etcd-ca no etcd-peer Jun 09, 2022 09:36 UTC 345d etcd-ca no etcd-server Jun 09, 2022 09:36 UTC 345d etcd-ca no front-proxy-client Jun 09, 2022 09:36 UTC 345d front-proxy-ca no scheduler.conf Jun 09, 2022 09:36 UTC 345d no CERTIFICATE AUTHORITY EXPIRES RESIDUAL TIME EXTERNALLY MANAGED ca Jun 07, 2031 09:36 UTC 9y no etcd-ca Jun 07, 2031 09:36 UTC 9y no front-proxy-ca Jun 07, 2031 09:36 UTC 9y no 可以看到CA证书有效期为10年，服务证书为1年（由于已经创建一段时间，有效期是向下取整的）。\nfanux同学已经总结了一篇修改kubeadm延长证书的文章：kubeadm定制化开发，延长证书，但按这篇文章修改之后，我发现几个问题：\n 修改之后的kubeadm仅延长了CA证书，对服务证书并没有作用 没有考虑使用bootstrap方式的kubelet client证书有效期  经过一番探索，在fanux基础上我们成功的实现了全集群的证书有效期延长，具体步骤如下：\n 修改CA有效期，这是fanux文章中提到过的 修改服务证书有效期 配置kube-controller-manager，为kubelet颁发99年有效期证书  1. 修改CA有效期 修改vendor/k8s.io/client-go/util/cert/cert.go\ndiff --git a/staging/src/k8s.io/client-go/util/cert/cert.go b/staging/src/k8s.io/client-go/util/cert/cert.go index 3da1441..37f5823 100644 --- a/staging/src/k8s.io/client-go/util/cert/cert.go +++ b/staging/src/k8s.io/client-go/util/cert/cert.go @@ -36,6 +36,7 @@ import (  ) const duration365d = time.Hour * 24 * 365 +const longYear = 100  // Config contains the basic fields required for creating a certificate type Config struct { @@ -63,7 +64,7 @@ func NewSelfSignedCACert(cfg Config, key crypto.Signer) (*x509.Certificate, erro  Organization: cfg.Organization, }, NotBefore: now.UTC(), - NotAfter: now.Add(duration365d * 10).UTC(), + NotAfter: now.Add(duration365d * longYear).UTC(),  KeyUsage: x509.KeyUsageKeyEncipherment | x509.KeyUsageDigitalSignature | x509.KeyUsageCertSign, BasicConstraintsValid: true, IsCA: true, @@ -93,7 +94,7 @@ func GenerateSelfSignedCertKey(host string, alternateIPs []net.IP, alternateDNS  // Certs/keys not existing in that directory are created. func GenerateSelfSignedCertKeyWithFixtures(host string, alternateIPs []net.IP, alternateDNS []string, fixtureDirectory string) ([]byte, []byte, error) { validFrom := time.Now().Add(-time.Hour) // valid an hour earlier to avoid flakes due to clock skew - maxAge := time.Hour * 24 * 365 // one year self-signed certs + maxAge := duration365d * longYear // one year self-signed certs  baseName := fmt.Sprintf(\u0026#34;%s_%s_%s\u0026#34;, host, strings.Join(ipsToStrings(alternateIPs), \u0026#34;-\u0026#34;), strings.Join(alternateDNS, \u0026#34;-\u0026#34;)) certFixturePath := filepath.Join(fixtureDirectory, baseName+\u0026#34;.crt\u0026#34;) @@ -107,7 +108,7 @@ func GenerateSelfSignedCertKeyWithFixtures(host string, alternateIPs []net.IP, a  } return nil, nil, fmt.Errorf(\u0026#34;cert %s can be read, but key %s cannot: %v\u0026#34;, certFixturePath, keyFixturePath, err) } - maxAge = 100 * time.Hour * 24 * 365 // 100 years fixtures + maxAge = duration365d * longYear // 100 years fixtures  } caKey, err := rsa.GenerateKey(cryptorand.Reader, 2048) 2. 修改服务证书有效期 修改cmd/kubeadm/app/util/pkiutil/pki_helpers.go中使用的变量kubeadmconstants.CertificateValidity\ncertTmpl := x509.Certificate{ Subject: pkix.Name{ CommonName: cfg.CommonName, Organization: cfg.Organization, }, DNSNames: cfg.AltNames.DNSNames, IPAddresses: cfg.AltNames.IPs, SerialNumber: serial, NotBefore: caCert.NotBefore, NotAfter: time.Now().Add(kubeadmconstants.CertificateValidity).UTC(), KeyUsage: x509.KeyUsageKeyEncipherment | x509.KeyUsageDigitalSignature, ExtKeyUsage: cfg.Usages, } 该变量定义在cmd/kubeadm/app/constants/constants.go\ndiff --git a/cmd/kubeadm/app/constants/constants.go b/cmd/kubeadm/app/constants/constants.go index 2e3f4c9..768cf70 100644 --- a/cmd/kubeadm/app/constants/constants.go +++ b/cmd/kubeadm/app/constants/constants.go @@ -46,7 +46,7 @@ const (  TempDirForKubeadm = \u0026#34;tmp\u0026#34; // CertificateValidity defines the validity for all the signed certificates generated by kubeadm - CertificateValidity = time.Hour * 24 * 365 + CertificateValidity = time.Hour * 24 * 365 * 100  // CACertAndKeyBaseName defines certificate authority base name CACertAndKeyBaseName = \u0026#34;ca\u0026#34; 3. 修改kube-controller-manager \u0026ndash;cluster-signing-duration kube-controller-manager的--cluster-signing-duration参数控制每次给kubelet颁发证书的有效期，默认值为8760h0m0s，也即1年。\n \u0026ndash;cluster-signing-duration duration Default: 8760h0m0s The length of duration signed certificates will be given.\n 可以在使用kubeadm初始化时配置kube-controllermanager参数，或初始化之后修改manifests的方式，修改--cluster-signing-duration值为876000h。\n4. 验证 最后验证一下，通过make all WHAT=cmd/kubeadm GOFLAGS=-v编译kubeadm。 使用新的kubeadm初始化或重新生成集群证书（执行kubeadm certs renew all不能更新CA）进行证书更新。 再次查看证书：\nCERTIFICATE EXPIRES RESIDUAL TIME CERTIFICATE AUTHORITY EXTERNALLY MANAGED admin.conf Jun 05, 2121 09:18 UTC 99y no apiserver Jun 05, 2121 09:32 UTC 99y ca no apiserver-etcd-client Jun 05, 2121 09:32 UTC 99y etcd-ca no apiserver-kubelet-client Jun 05, 2121 09:32 UTC 99y ca no controller-manager.conf Jun 05, 2121 09:18 UTC 99y no etcd-healthcheck-client Jun 05, 2121 09:32 UTC 99y etcd-ca no etcd-peer Jun 05, 2121 09:32 UTC 99y etcd-ca no etcd-server Jun 05, 2121 09:32 UTC 99y etcd-ca no front-proxy-client Jun 05, 2121 09:32 UTC 99y front-proxy-ca no scheduler.conf Jun 05, 2121 09:18 UTC 99y no CERTIFICATE AUTHORITY EXPIRES RESIDUAL TIME EXTERNALLY MANAGED ca Jun 05, 2121 09:32 UTC 99y no etcd-ca Jun 05, 2121 09:32 UTC 99y no front-proxy-ca Jun 05, 2121 09:32 UTC 99y no 100年证书稳如老狗！！\n"
            }
        
    ,
        
            {
                "ref": "https://xiaosuiba.github.io/2021/06/02/how-does-kube-public-works/",
                "title": "kube-public命名空间工作原理",
                "section": "post",
                "date" : "2021.06.02",
                "body": "kube-public中的资源可以被所有用户读取，本文探索了其实现方式和当前的现状。\nkube-public命名空间工作原理 在开发中，我们使用到了kube-public命名空间中的cluster-info configmap。按照官方文档的说法，kube-public是任何用户（包括未认证用户）都可以访问的，官网的说明如下：\n kube-public This namespace is created automatically and is readable by all users (including those not authenticated). This namespace is mostly reserved for cluster usage, in case that some resources should be visible and readable publicly throughout the whole cluster. The public aspect of this namespace is only a convention, not a requirement.\n 而我们在实际测试当中，却发现使用kubeadm创建的集群中，匿名用户仅能实现对kube-public中cluster-info对象的get操作，而对该命名空间的其余动作或者操作其余资源均会返回403Forbidden，这显然与官网文档说法有出入。那么kube-public命名空间到底怎么实现的呢？我们进行了一番探索。\n首先，从kubernetes源码当中，进行一番搜索，找到两处可疑的代码： plugin/pkg/auth/authorizer/rbac/bootstrappolicy/namespace_policy.go#L140\naddNamespaceRole(metav1.NamespacePublic, rbacv1.Role{ // role for the bootstrap signer to be able to write its configmap \tObjectMeta: metav1.ObjectMeta{Name: saRolePrefix + \u0026#34;bootstrap-signer\u0026#34;}, Rules: []rbacv1.PolicyRule{ rbacv1helpers.NewRule(\u0026#34;get\u0026#34;, \u0026#34;list\u0026#34;, \u0026#34;watch\u0026#34;).Groups(legacyGroup).Resources(\u0026#34;configmaps\u0026#34;).RuleOrDie(), rbacv1helpers.NewRule(\u0026#34;update\u0026#34;).Groups(legacyGroup).Resources(\u0026#34;configmaps\u0026#34;).Names(\u0026#34;cluster-info\u0026#34;).RuleOrDie(), eventsRule(), }, }) addNamespaceRoleBinding(metav1.NamespacePublic, rbacv1helpers.NewRoleBinding(saRolePrefix+\u0026#34;bootstrap-signer\u0026#34;, metav1.NamespacePublic).SAs(metav1.NamespaceSystem, \u0026#34;bootstrap-signer\u0026#34;).BindingOrDie()) 此处创建了一个角色saRolePrefix + \u0026quot;bootstrap-signer\u0026quot;，并和名为bootstrap-signer的sa进行绑定。看起来和匿名访问关系不大。\ncmd/kubeadm/app/phases/bootstraptoken/clusterinfo/clusterinfo.go#L85\n// CreateClusterInfoRBACRules creates the RBAC rules for exposing the cluster-info ConfigMap in the kube-public namespace to unauthenticated users func CreateClusterInfoRBACRules(client clientset.Interface) error { klog.V(1).Infoln(\u0026#34;creating the RBAC rules for exposing the cluster-info ConfigMap in the kube-public namespace\u0026#34;) err := apiclient.CreateOrUpdateRole(client, \u0026amp;rbac.Role{ ObjectMeta: metav1.ObjectMeta{ Name: BootstrapSignerClusterRoleName, Namespace: metav1.NamespacePublic, }, Rules: []rbac.PolicyRule{ { Verbs: []string{\u0026#34;get\u0026#34;}, APIGroups: []string{\u0026#34;\u0026#34;}, Resources: []string{\u0026#34;configmaps\u0026#34;}, ResourceNames: []string{bootstrapapi.ConfigMapClusterInfo}, }, }, }) if err != nil { return err } return apiclient.CreateOrUpdateRoleBinding(client, \u0026amp;rbac.RoleBinding{ ObjectMeta: metav1.ObjectMeta{ Name: BootstrapSignerClusterRoleName, Namespace: metav1.NamespacePublic, }, RoleRef: rbac.RoleRef{ APIGroup: rbac.GroupName, Kind: \u0026#34;Role\u0026#34;, Name: BootstrapSignerClusterRoleName, }, Subjects: []rbac.Subject{ { Kind: rbac.UserKind, Name: user.Anonymous, }, }, }) } 这段代码首先创建了名为BootstrapSignerClusterRoleName也就是kubeadm:bootstrap-signer-clusterinfo的角色，该角色对kube-public中的cluster-info configmap具有get权限。然后创建了一个绑定，将上面的角色与user.Anonymous绑定起来，这样实现任意用户对cluster-info的get操作授权。同时也说明了为什么未授权用户不能读取其余自定义的资源。\n一切都没有魔法，只是kubeadm初始化集群时进行了权限配置。再次看一遍官方的文档，其中有一句：is only a convention。这仅仅是一个约定，系统不会帮你实现这个约定，而是需要用户自己遵守并实现。\n"
            }
        
    ,
        
            {
                "ref": "https://xiaosuiba.github.io/2021/05/14/container-storage-interface/",
                "title": "Kubernet CSI Volume 插件设计文档(译)",
                "section": "post",
                "date" : "2021.05.14",
                "body": "本文是对CSI Volume Plugins in Kubernetes Design Doc的翻译\nKubernetes CSI Volume 插件设计文档 状态 挂起\n版本: Alpha\n作者： Saad Ali (@saad-ali, saadali@google.com)\n本文草稿在此.\n术语    术语 定义     容器存储接口（Container Storage Interface, CSI） 一个尝试建立行业标准接口的规范，容器编排系统（CO）可以使用该接口将任意存储系统暴露于其容器化工作负载中   树内（in-tree） 代码位于Kubernetes核心代码库中   树外（out-of-tree） 代码位于Kubernetes核心代码库外。   CSI 卷插件（CSI Volume Plugin） 一种新的、树内的卷插件，作为一种适配器，允许在Kubernetes中使用其余树外的第三方CSI卷驱动。   CSI卷驱动（CSI Volume Driver） 一个树外的卷插件兼容实现，可以在Kubernetes中通过CSI卷插件来使用。    背景和动机 当前的Kubernetes卷插件为“树内”，表明他们将和Kubernetes核心二进制一起链接、编译、构建和发布。添加一种新的存储系统到Kubernetes中（一种卷插件）需要将代码检入Kubernetes核心代码库中。许多原因导致这样做并不理想：\n 卷插件开发依赖Kubernetes发行版并与其紧耦合。 Kubernetes开发者/社区负责所有卷插件的测试和维护，而不仅仅是一个稳定的插件API。 卷插件中的缺陷可能使Kubernetes组件崩溃，而不仅仅是插件本身。 卷插件对Kubernetes组件（kubelet 和 kube-controller-manager）具有完全权限. 插件开发者将被迫开源插件代码，而不能选择仅仅发布一个二进制。  现有的Flex Volume插件尝试通过暴露一个基于可执行文件的挂载/卸载/附加/分离（mount/unmount/attach/detach） API来解决这个问题。虽然他允许第三方存储厂商编写树外的驱动，但仍然要求有访问node和master节点机器的root文件系统的权限，以部署第三方驱动文件。\n而且，这并没有解决树内卷插件的另一个痛点：依赖。卷驱动往往有很多外部需求：例如依赖挂载和文件系统工具。这些插件假设底层宿主机上的依赖已经存在，但这通常并非事实。而且安装他们也需要直接访问机器。目前已经有一些希望解决树内卷插件问题的努力正在进行中，例如https://github.com/kubernetes/community/pull/589。但是，使卷插件完全容器化将使得依赖管理更加容易。\n虽然Kubernetes一直在处理这些问题，泛存储社区也一直在探讨如何使他们的存储系统在不同容器编排系统（CO）中可用的碎片化story。存储厂商只能选择为不同的容器编排系统提供多个卷驱动，或者选择不支持某些容器编排系统。\n容器存储接口（Container Storage Interface，CSI）是来自不同容器编排系统社区成员之间合作的规范，这些成员包括Kubernetes、Mesos、Cloud Foundry和Docker。该接口的目标是为CO建立一种标准化机制，以将任意存储系统暴露于其容器化工作负载中。\n存储供应商采用该接口的主要动机是希望以尽可能少的工作使他们的系统对更多的用户可用。CO使用该接口的主要动机是投资一种机制，使他们的用户可以使用更多的存储系统。此外，对于Kubernetes而言，采用CSI将带来将卷插件移出树内并支持卷插件容器化的额外好处。\n链接  容器存储接口（Container Storage Interface，CSI）规范  目的 本文的目的是记录在Kubernetes中启用CSI兼容插件（CSI卷驱动程序）的所有需求。\n目标  定义Kubernetes API，以便与任意第三方CSI卷驱动程序进行交互。 定义一种机制，Kubernetes master和node组件将通过该机制与任意的第三方CSI卷驱动程序安全地进行通信。 定义一种机制，Kubernetes master和node组件将通过该机制发现并注册在Kubernetes上部署的任意第三方CSI卷驱动程序。 为兼容Kubernetes的第三方CSI卷驱动程序的打包要求提供建议。 为部署在Kubernetes集群上与之兼容的第三方CSI卷驱动程序的部署过程提供建议。  非目标  替换 [Flex卷插件]  Flex卷插件将作为一种基于可执行文件机制，可以创建“树外”卷插件的存在。 由于存在Flex驱动程序并依赖Flex接口，因此它将继续被稳定的API支持。 CSI卷插件将于Flex卷插件共存。    设计概述 为了支持CSI兼容卷插件，Kubernetes中将引入一个新的树内 CSI卷插件。这个新的卷插件将成为Kubernetes用户（应用程序开发人员和集群管理员）与外部CSI卷驱动程序进行交互的机制。\n新的树内CSI卷插件的SetUp/TearDown调用将通过节点计算机上的unix域套接字直接调用NodePublishVolume和NodeUnpublishVolume CSI RPC。\n制备/删除和附加/分离操作必须由某些外部组件处理，该组件代表CSI卷驱动程序监听Kubernetes API，并恰当的调用CSI RPC。\n为了简化集成，Kubernetes团队将提供一个容器，该容器可捕获所有Kubernetes特定的逻辑，并充当第三方容器化CSI卷驱动程序和Kubernetes之间的适配器（每个CSI驱动程序部署都具有其自己的适配器实例）。\n设计细节 第三方CSI插件驱动 Kubernetes将尽量减少CSI卷驱动的打包和部署的说明。在Kubernetes中启用任意一个外部CSI兼容存储驱动的需求是使用上文提及的“通信通道”。\n本文推荐了一种用于在Kubernetes上部署任意容器化CSI驱动程序的标准机制。存储提供商可以使用它来简化在Kubernetes上容器化部署CSI兼容卷驱动程序（请参阅下面的“在Kubernetes上部署CSI驱动程序的推荐机制”部分）。但是，此机制仅作为推荐，而不是严格的必选项。\n通信通道 Kubelet到CSI驱动的通信 Kubelet（负责挂载和卸载）将通过一个Unix域套接字与一个运行在相同主机上的“CSI卷驱动”进行通信。\nCSI卷驱动应该在主机节点下列路径中创建一个套接字：/var/lib/kubelet/plugins/[SanitizedCSIDriverName]/csi.sock。在alpha版本中，kubelet将假设这是与CSI卷驱动进行交谈Unix域套接字。在beta实现中，我们可以考虑使用设备插件Unix域套接字机制来向kubelet注册Unix域套接字。这个机制将被扩展以同时独立支持CSI卷插件和设备插件。\n“Sanitized CSIDriverName”是不包含危险字符的CSI驱动程序名称，并可以用作annotation名称。可以遵循与volume plugins相同的模式。太长或太丑陋的驱动程序名称都可以被拒绝，这种情况下本文档中描述的所有组件都将报告错误，并且不会与此CSI驱动程序通信。确切的命名方法属于实施细节（在最坏的情况下为SHA）。\n在初始化外部“CSI卷驱动程序”时，kubelet必须调用CSI方法NodeGetInfo以获取从Kubernetes节点名称到CSI驱动程序NodeID以及相关的accessible_topology的映射。它必须：\n  使用来自accessible_topology的拓扑键（topology keys）和NodeID为节点创建/更新CSINodeInfo对象实例。\n 这将使发出ControllerPublishVolume调用的组件能够使用CSINodeInfo作为从集群节点ID到存储节点ID的映射。 这将使发出CreateVolume的组件能够重建accessible_topology并提供可从特定节点访问的卷。 每个驱动程序必须完全覆盖其先前版本的NodeID和拓扑键（如果存在）。 如果NodeGetInfo调用失败，则kubelet必须删除该驱动程序的所有以前的NodeID和拓扑键。 实施kubelet插件注销机制后，请在注销驱动程序时删除NodeID和拓扑键。    使用CSI驱动程序的NodeID更新Node API对象的csi.volume.kubernetes.io/nodeid annotation。Annotation的值是一个JSON Blob，其中包含每个CSI驱动程序的键/值对。例如：\ncsi.volume.kubernetes.io/nodeid: \u0026quot;{ \\\u0026quot;driver1\\\u0026quot;: \\\u0026quot;name1\\\u0026quot;, \\\u0026quot;driver2\\\u0026quot;: \\\u0026quot;name2\\\u0026quot; } 该annotation已弃用，并将根据弃用政策（弃用后1年）将其删除。TODO：标注启用的date.\n 如果NodeGetInfo调用失败，则kubelet必须删除该驱动程序的所有以前的NodeID。 实施kubelet插件注销机制后，请在注销驱动程序时删除NodeID和拓扑键。    以accessible_topology作为标签创建/更新Node API对象。 标签格式没有硬性限制，但是对于推荐设置使用的格式，请参考节点对象中的拓扑表示。\n  为了简化容器化部署外部CSI卷驱动程序，Kubernetes团队将提供一个辅助工具“ Kubernetes CSI Helper”容器，该容器可以管理Unix域套接字注册和NodeId初始化。下面的“在Kubernetes上部署CSI驱动程序的建议机制”部分对此进行了详细说明。\n名为CSINodeInfo的新API对象将定义如下：\n// CSINodeInfo包含有关节点上安装的所有CSI驱动程序的状态的信息 type CSINodeInfo struct { metav1.TypeMeta // ObjectMeta.Name 必须是一个节点名  metav1.ObjectMeta // 在节点上运行的CSI驱动程序及其属性的列表。  CSIDrivers []CSIDriverInfo } // 有关节点上安装的一个CSI驱动程序的信息。 type CSIDriverInfo struct { // CSI驱动名称  Name string // 从驱动程序的角度来看的节点ID。  NodeID string // 驱动程序在节点上报告的拓扑键。  TopologyKeys []string } 选择使用一个新对象类型CSINodeInfo而不是Node.Status字段，是因为Node已经足够大了，再增加其大小将带来问题。CSINodeInfo是由TODO（jsafrane）在集群启动时安装的CRD，并在kubernetes/kubernetes/pkg/apis/storage-csi/v1alpha1/types.go中定义，因此会自动生成k8s.io/client-go和k8s.io/ api。如果未安装CRD，则CSINodeInfo所有用户都会容忍，并对任何需要他们的事物进行指数退避重试并恰当的报告错误。尤其是kubelet，在缺少CRD的情况下也能够履行其通常的职责。\n每个节点必须有零个或一个CSINodeInfo实例。这通过CSINodeInfo.Name == Node.Name进行保证。TODO：如何对此进行验证？每个CSINodeInfo被对应的节点“拥有”以进行垃圾收集。\nMaster到CSI驱动的通信 由于CSI卷驱动程序代码被认为是不受信任的，因此它可能不被允许在master上运行。因此，Kube控制器管理器（负责创建，删除，附加和分离）不能通过Unix域套接字与“CSI卷驱动”容器进行通信，而需要通过Kubernetes API来完成通信过程。\n更具体地说，某些外部组件必须代表外部CSI卷驱动程序监听Kubernetes API并对其触发适当的操作。这解决了发现和保护kube-controller-manager与CSI卷驱动器之间的通道的问题。\n为了在Kubernetes上轻松部署外部容器化CSI卷驱动程序同时使其不感知Kubernetes，Kubernetes将提供一个辅助工具“Kubernetes to CSI”代理容器，该容器将监听Kubernetes API并触发“CSI卷驱动程序”容器进行适当的操作“ 。下面的“在Kubernetes上部署CSI驱动程序的建议机制”部分对此进行了详细说明。\n代表外部CSI卷驱动程序监听Kubernetes API的外部组件必须处理制备、删除、附件和分离操作。\n制备和删除 制备和删除操作使用了现有的provisioner机制，在这个过程中，代表外部CSI卷驱动程序监听Kubernetes API的外部组件表现得像一个provisioner。\n简而言之，为了动态地制备新的CSI卷，集群管理员将创建一个StorageClass，其provisioner与作为CSI卷驱动处理制备请求的外部provisioner的名称相对应。\n为了制备新的CSI卷，终端用户将引用该StorageClass创建一个PersistentVolumeClaim对象。外部provisioner将对PVC的创建做出反应，并针对CSI卷驱动程序发出CreateVolume调用以制备该卷。CreateVolume名称将自动生成，就像其他动态预配置的卷一样。CreateVolume的容量将取自PersistentVolumeClaim对象。CreateVolume参数将从StorageClass参数传递（对Kubernetes不透明）。\n如果PersistentVolumeClaim具有volume.alpha.kubernetes.io/selected-node annotation（仅当在StorageClass中启用了延迟卷绑定时才添加），provisioner将从相应的CSINodeInfo实例中获取相关的拓扑键，并从Node标签获取拓扑值，然后使用它们在 CreateVolume()请求中生成首选拓扑。如果未设置annotation，则不会指定首选拓扑（除非PVC遵循StatefulSet命名格式，这将在本节后面讨论）。StorageClass中的的AllowedTopologies将作为必须拓扑传递。如果未指定AllowedTopologies，provisioner将在整个集群中传递一组聚合的拓扑值作为必需的拓扑。\n为了执行此拓扑聚合，外部provisioner将缓存所有现有的Node对象。为了防止受损的节点影响制备过程，它将选择单个节点作为键的真实来源，而不是依赖节点对象存储在CSINodeInfo中的键。对于晚绑定的PVC，将使用选择的节点；否则将选择一个随机节点。然后，provisioner将遍历所有包含来自驱动程序的节点ID的缓存节点，并使用这些键聚合标签。请注意，如果整个集群中的拓扑键不同，则仅考虑与所选节点的拓扑键匹配的节点子集进行配置。\n为了生成首选拓扑，外部provisioner将在 CreateVolume() 调用中为首选拓扑生成N个段，其中N是必需拓扑的大小。包含多个段则可以支持跨多个拓扑段可用的卷。所选节点中的拓扑段将始终是首选拓扑中的第一个。所有其他段都是对其余必要拓扑的一些重新排序，以便在给定必要拓扑（或其任意任意重新排序）和选定节点的情况下，可以保证首选拓扑的集合始终相同。\n如果设置了即时卷绑定模式，并且PVC遵循StatefulSet命名格式，则provisioner将根据PVC名称从必需拓扑中选择一个段作为首选拓扑中的第一段，以确保StatefulSet的卷在拓扑上均匀分布。该逻辑类似于GCE Persistent Disk provisioner中的名称哈希逻辑。优选拓扑中的其他段以与上述相同的方式排序。此功能将在建议的部署方法的一部分中提供的外部provisioner中进行标记。\n一旦操作成功完成，外部provisioner就会使用 CreateVolume 响应中返回的信息创建一个PersistentVolume对象来表示该卷。返回的卷的拓扑将转换为PersistentVolume的NodeAffinity字段。然后，PersistentVolume对象将绑定到PersistentVolumeClaim并可供使用。\n拓扑键/值对的格式由用户定义，并且必须在以下位置匹配：\n Node拓扑标签 PersistentVolume的NodeAffinity字段 StorageClass的AllowedTopologies字段 当StorageClass启用了延迟卷绑定时，调度程序将通过以下方式使用Node的拓扑信息：  在动态制备期间，调度程序通过将每个Node的拓扑与StorageClass中的AllowedTopologies进行比较，为provisioner选择一个候选节点。 在卷绑定和Pod调度期间，调度程序通过比较Node拓扑与PersistentVolume中的VolumeNodeAffinity来为Pod选择一个候选节点。    您可以在拓扑感知的卷调度设计文档中找到更详细的描述。有关建议的部署方法使用的格式，请参见[节点对象中的拓扑表示]](#topology-representation-in-node-objects)。\n要删除CSI卷，终端用户需要删除相应的PersistentVolumeClaim对象。外部provisioner将对PVC的删除做出反应，并根据其回收策略，对CSI卷驱动程序命令发出DeleteVolume调用以删除该卷。然后它将删除 PersistentVolume对象。\n附加和分离 附加/分离操作也必须由外部组件（“attacher”）处理。attacher代表外部CSI卷驱动程序监听Kubernetes API中的新VolumeAttachment对象（定义如下），并触发对CSI卷驱动程序的适当调用以附加该卷。attacher必须监听 VolumeAttachment对象，并将其标记为已附加，即使底层CSI驱动程序不支持ControllerPublishVolume调用，因为Kubernetes并不知道细节。\n更具体地说，外部“attacher”必须代表外部CSI卷驱动程序监听Kubernetes API，以处理附加/分离请求。\n一旦满足以下条件，external-attacher应针对CSI卷驱动程序调用ControllerPublishVolume以将卷附加到指定的节点：\n Kubernetes 附加/分离控制器创建了一个新的VolumeAttachment Kubernetes API对象。 该对象的VolumeAttachment.Spec.Attacher值与外部attacher对应。 VolumeAttachment.Status.Attached还未设置为true。   存在名称与VolumeAttachment.Spec.NodeName匹配的Kubernetes Node API对象，并且该对象包含csi.volume.kubernetes.io/nodeidannotation。该annotation包含一个JSON Blob，他是一个键/值对列表，其中的键之一与CSI卷驱动程序名称相对应，并且值是该驱动程序的NodeID。这个NodeId映射可以在ControllerPublishVolume调用中取出并使用。 或者，存在名称与 VolumeAttachment.Spec.NodeName匹配的CSINodeInfo API对象，并且该对象包含用于CSI卷驱动程序的CSIDriverInfo。CSIDriverInfo包含用于ControllerPublishVolume调用的NodeID。   未设置 VolumeAttachment.Metadata.DeletionTimestamp。  在开始ControllerPublishVolume操作之前，external-attacher应将以下finalizers添加到以下Kubernetes API对象中：\n  添加到VolumeAttachment 上，以便在删除对象时，external-attacher有机会首先分离该卷。一旦将卷从节点上完全分离，外部attacher将删除此finalizer。\n  附加VolumeAttachment引用的PersistentVolume，因此在该卷被附加时无法删除PV。外部attacher需要来自PV的信息来执行分离操作。一旦所有引用PV的VolumeAttachment对象都被删除，即卷从所有节点分离，则attacher将删除finalizer。\n  如果操作成功完成，则external-attacher将：\n 将VolumeAttachment.Status.Attached字段设置为true表示已附加该卷。 使用返回的PublishVolumeInfo的内容更新VolumeAttachment.Status.AttachmentMetadata字段。 清除VolumeAttachment.Status.AttachError字段。  如果操作失败，external-attacher将会：\n 确保VolumeAttachment.Status.Attached字段仍为false，以指示未附加卷。 设置详细说明错误的VolumeAttachment.Status.AttachError字段。 针对与VolumeAttachment对象相关联的Kubernetes API创建一个事件，以通知用户出了什么问题。  external-attacher可以实施自己的错误恢复策略，并在上面指定的附加条件有效的情况下重试。强烈建议external-attacher对重试执行指数退避策略。\n分离操作将通过删除VolumeAttachment Kubernetes API对象来触发。由于external-attacher将为VolumeAttachment Kubernetes API对象添加一个finalizer，因此在删除该对象之前将等待来自external-attacher的确认。\n一旦满足以下所有条件，则external-attacher应针对CSI卷驱动程序调用ControllerUnpublishVolume，以将卷与指定节点分离：\n 将VolumeAttachment Kubernetes API对象标记为删除：VolumeAttachment.metadata.deletionTimestamp字段的值已设置。  如果操作成功完成，则external-attacher将：\n 从VolumeAttachment对象上的finalizer列表中删除其finalizer，以允许继续进行删除操作。  如果操作失败，external-attacher将会：\n 确保 VolumeAttachment.Status.Attached字段保持为true，以指示尚未分离卷。 设置VolumeAttachment.Status.DetachError字段以详细说明错误。 针对与VolumeAttachment对象相关联的Kubernetes API创建一个事件，以通知用户出了什么问题。  名为VolumeAttachment 的新API对象定义如下：\n// VolumeAttachment用于捕获从特定节点附加或分离特定卷的意图 // // VolumeAttachment对象是non-namespaced的。 type VolumeAttachment struct { metav1.TypeMeta `json:\u0026#34;,inline\u0026#34;` // 标准object metadata. \t// 更多信息：https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata \t// +optional \tmetav1.ObjectMeta `json:\u0026#34;metadata,omitempty\u0026#34; protobuf:\u0026#34;bytes,1,opt,name=metadata\u0026#34;` // 所需的附加/分离卷行为的规格。 \t// 由Kubernetes系统填充。 \tSpec VolumeAttachmentSpec `json:\u0026#34;spec\u0026#34; protobuf:\u0026#34;bytes,2,opt,name=spec\u0026#34;` // VolumeAttachment请求的状态。 \t// 由完成附加或分离操作的实体（即external-attacher）填充。 \t// +optional \tStatus VolumeAttachmentStatus `json:\u0026#34;status,omitempty\u0026#34; protobuf:\u0026#34;bytes,3,opt,name=status\u0026#34;` } // VolumeAttachment请求的规格。 type VolumeAttachmentSpec struct { // Attacher 指明必须处理此请求的卷驱动器的名称。\t// 这是GetPluginName()返回的名称，必须是 \t// 与StorageClass.Provisioner相同。 \tAttacher string `json:\u0026#34;attacher\u0026#34; protobuf:\u0026#34;bytes,1,opt,name=attacher\u0026#34;` // AttachedVolumeSource代表需要附加的卷。 \tVolumeSource AttachedVolumeSource `json:\u0026#34;volumeSource\u0026#34; protobuf:\u0026#34;bytes,2,opt,name=volumeSource\u0026#34;` // 卷需要附加到的Kubernetes节点名称。 \tNodeName string `json:\u0026#34;nodeName\u0026#34; protobuf:\u0026#34;bytes,3,opt,name=nodeName\u0026#34;` } // VolumeAttachmentSource表示需要被附加的卷。 // 目前只有PersistentVolumes可以通过外部附加器附加， // 未来我们还可以允许附加pod内联卷。 // 仅能设置一个成员 type AttachedVolumeSource struct { // 要附加的持久卷名称 \t// +optional \tPersistentVolumeName *string `json:\u0026#34;persistentVolumeName,omitempty\u0026#34; protobuf:\u0026#34;bytes,1,opt,name=persistentVolumeName\u0026#34;` // *VolumeSource的占位符，以容纳Pod中的内联卷。 } // VolumeAttachment请求的状态。 type VolumeAttachmentStatus struct { // 表明卷被成功附加。 \t// 这个字段只能被完成附加动作的实体设置，也即外部附加器。 \tAttached bool `json:\u0026#34;attached\u0026#34; protobuf:\u0026#34;varint,1,opt,name=attached\u0026#34;` // 成功执行附加后，该字段将填充附加操作返回的所有信息，这些信息必须传递到后续的WaitForAttach或Mount调用中。 \t// 这个字段只能被完成附加动作的实体设置，也即外部附加器。 \t// +optional \tAttachmentMetadata map[string]string `json:\u0026#34;attachmentMetadata,omitempty\u0026#34; protobuf:\u0026#34;bytes,2,rep,name=attachmentMetadata\u0026#34;` // The most recent error encountered during attach operation, if any. \t// 这个字段只能被完成附加动作的实体设置，也即外部附加器。 \t// +optional  AttachError *VolumeError `json:\u0026#34;attachError,omitempty\u0026#34; protobuf:\u0026#34;bytes,3,opt,name=attachError,casttype=VolumeError\u0026#34;` // 分离操作期间遇到的最新错误（如果有）。 \t// 这个字段只能被完成分离动作的实体设置，也即外部附加器。 \t// +optional \tDetachError *VolumeError `json:\u0026#34;detachError,omitempty\u0026#34; protobuf:\u0026#34;bytes,4,opt,name=detachError,casttype=VolumeError\u0026#34;` } // 捕获在卷操作期间遇到的错误。 type VolumeError struct { // 遇到错误的时间。 \t// +optional \tTime metav1.Time `json:\u0026#34;time,omitempty\u0026#34; protobuf:\u0026#34;bytes,1,opt,name=time\u0026#34;` // 详细描述在附加或分离操作期间遇到的错误的字符串。 \t// 该字符串可能已被记录，因此它不应包含敏感信息。 \t// +optional \tMessage string `json:\u0026#34;message,omitempty\u0026#34; protobuf:\u0026#34;bytes,2,opt,name=message\u0026#34;` } Kubernetes 树内 CSI卷插件 新的树内Kubernetes CSI卷插件将包含Kubernetes与任意树外第三方CSI兼容卷驱动程序进行通信所需的所有逻辑。\n现有的Kubernetes卷组件（附加/分离控制器、PVC/PV控制器、Kubelet卷管理器） 将向对待现有的树内卷插件一样处理这个CSI卷插件的生命周期操作（包括触发卷的创建/删除、附加/分离以及挂载/卸载）。\n拟议的API 新的CSIPersistentVolumeSource对象将被添加到Kubernetes API。它将是现有PersistentVolumeSource对象的一部分，因此只能通过PersistentVolume使用。Pod不允许在没有PersistentVolumeClaim的情况下直接引用CSI卷。\ntype CSIPersistentVolumeSource struct { // Driver是用于该卷的驱动程序的名称。  // 必须。  Driver string `json:\u0026#34;driver\u0026#34; protobuf:\u0026#34;bytes,1,opt,name=driver\u0026#34;` // VolumeHandle是CSI卷返回的唯一卷名  // 插件的CreateVolume，后续所有调用中的卷都将引用。  VolumeHandle string `json:\u0026#34;volumeHandle\u0026#34; protobuf:\u0026#34;bytes,2,opt,name=volumeHandle\u0026#34;` // 可选： 传递给ControllerPublishVolumeRequest的值。  // 默认为false（读/写）。 // +optional  ReadOnly bool `json:\u0026#34;readOnly,omitempty\u0026#34; protobuf:\u0026#34;varint,5,opt,name=readOnly\u0026#34;` } 内部接口 树内 CSI卷插件将实现以下内部Kubernetes卷接口：\n VolumePlugin  从特定路径挂载/卸载一个卷   AttachableVolumePlugin  从给定节点附加/分离一个卷。    值得注意的是，ProvisionableVolumePlugin和DeletableVolumePlugin没有实现，这是因为CSI卷的制备和删除是由外部设置程序处理的。\n挂载和卸载 树内卷插件的SetUp和TearDown方法将通过Unix域套接字触发NodePublishVolume和NodeUnpublishVolume CSI调用。Kubernetes将生成一个唯一的target_path（每个卷每个容器唯一），并通过NodePublishVolume传递给CSI插件以挂载该卷。成功完成NodeUnpublishVolume调用后（一旦卷卸载已被验证），Kubernetes将删除该目录。\nKubernetes卷子系统目前不支持块（仅文件），因此对于Alpha版本，Kubernetes CSI卷插件将仅支持文件。\n附加和分离 作为master上kube-controller-manager二进制文件的一部分运行的附加/分离控制器，决定何时必须从特定节点附加或分离CSI卷。\n当控制器决定附加CSI卷时，它将调用树内CSI卷插件的attach方法。树内CSI卷插件的attach方法将执行以下操作：\n 创建一个新的VolumeAttachment对象（在“通信通道”部分中定义）以附加该卷。  VolumeAttachment对象的名称为pv-\u0026lt;SHA256(PVName+NodeName)\u0026gt;.  设置pv-前缀是为了将来启用内联卷时能设置其他的前缀格式。 SHA256散列是为了减少PVName和NodeName字符串的长度，他们都可以是允许的最大名称长度（SHA256的十六进制表示形式为64个字符）。 PVName是附加的PersistentVolume的PV.name。 NodeName是卷应附加到的节点的Node.name。   如果已经存在具有相应名称的VolumeAttachment对象，则树内卷插件将按照下面的定义简单地开始对其进行轮询。该对象未被修改；只有外部代理才能更改状态字段；外部连接器负责其自己的重试和错误处理逻辑。   轮询VolumeAttachment对象，等待以下条件之一：  VolumeAttachment.Status.Attached字段变为true。  操作成功完成。   VolumeAttachment.Status.AttachError字段中设置了错误。  该操作以特定的错误终止。   操作超时。  该操作因超时错误而终止。   设置了VolumeAttachment.DeletionTimestamp。  一个错误终止了该操作，这个错误指示分离操作正在进行中。 不能信任VolumeAttachment.Status.Attached值。在创建对象的新实例之前，连接/分离控制器必须等到外部连接器删除了该对象。      当控制器决定分离CSI卷时，它将调用树内CSI卷插件的分离方法。树内CSI卷插件的分离方法将执行以下操作：\n 删除相应的VolumeAttachment对象（在“通信通道”部分中定义），以指示应分离该卷。 轮询VolumeAttachment对象，等待以下条件之一：  VolumeAttachment.Status.Attached字段变为false。  操作成功完成。   在VolumeAttachment.Status.DetachError字段中设置的错误。  该操作以特定的错误终止。   对象不再存在。  操作成功完成。   操作超时。  该操作因超时错误而终止。      在Kubernetes上部署CSI驱动程序的推荐机制 尽管Kubernetes并未规定CSI卷驱动程序的打包方式，但它提供以下建议以简化在Kubernetes上容器化CSI卷驱动程序的部署。\n要部署容器化的第三方CSI卷驱动程序，建议存储供应商：\n  创建一个CSI卷驱动程序容器，该容器实现卷插件的行为并通过CSI规范（包括Controller，Node和Identity服务）中定义的unix域套接字公开gRPC接口。\n  将CSI卷驱动程序容器与Kubernetes团队将提供的帮助程序容器（external-attacher, external-provisioner, node-driver-registrar, cluster-driver-registrar, external-resizer, external-snapshotter, livenessprobe）捆绑在一起，帮助器容器将协助CSI卷驱动器容器与Kubernetes系统进行交互。更具体地说，创建以下Kubernetes对象：\n  一个具有以下内容的StatefulSet或Deployment（取决于用户的需求;请参阅集群级部署） 以便与Kubernetes控制器的通信：\n  下列容器\n 由存储供应商创建的“CSI卷驱动程序”容器。 Kubernetes团队提供的容器（所有容器都是可选的）：  cluster-driver-registrar（有关何时需要容器的信息，请参见cluster-driver-registrar存储库中的README文件） external-provisioner（制备/删除操作所需） external-attacher（进行附加/分离操作所必需。如果您想跳过附加步骤，除了省略该容器外，还必须在Kubernetes中启用CSISkipAttach功能） external-resizer（调整大小操作所需） external-snapshotter（卷级快照操作所需） livenessprobe      以下卷：\n 一个emptyDir卷  由所有容器挂载（包括“ CSI卷驱动程序”）。 “CSI卷驱动程序”容器应在此目录中创建其Unix域套接字，以启用与Kubernetes帮助器容器的通信。      一个DaemonSet（以便与kubelet的每个实例进行通信）包含：\n 下列容器  由存储供应商创建的“CSI卷驱动程序”容器。 Kubernetes团队提供的容器：  node-driver-registrar-负责向kubelet注册unix域套接字。 livenessprobe （可选）     以下卷：  hostpath卷  暴露主机上的/var/lib/kubelet/plugins_registry。 仅在node-driver-registrar容器的/registration目录挂载 node-driver-registrar将使用此unix域套接字向kubelet注册CSI驱动程序的unix域套接字。   hostpath卷  暴露主机的/var/lib/kubelet/目录。 仅在 “CSI volume driver”的/var/lib/kubelet/目录挂载 确保启用了双向挂载传播 ，以便将此容器内的所有挂载设置传播回主机。   hostpath卷  使用hostPath.type =\u0026quot;DirectoryOrCreate\u0026quot;暴露主机上的/var/lib/kubelet/plugins/[SanitizedCSIDriverName]/。 在CSI卷驱动器容器内的CSI gRPC套接字创建路径上安装。 这是Kubelet与CSI卷驱动程序容器（gRPC over UDS）之间进行通信的主要方式。          让集群管理员部署上述StatefulSet和DaemonSet以在其Kubernetes集群中添加对存储系统的支持。\n  或者，可以通过将所有组件（包括external-provisioner和 external-attacher ）放在同一pod（DaemonSet）中来简化部署。但是，这样做会消耗更多资源，并且需要在external-provisioner和 external-attacher 组件中使用领导者选举协议 (参考https://git.k8s.io/contrib/election) 。\nKubernetes提供的容器在GitHub kubernetes-csi组织中维护。\n集群级部署 集群级部署中的容器可以采用以下配置之一进行部署：\n 具有单个副本的StatefulSet。适合具有单个专用节点的集群来运行集群级别的容器。StatefulSet保证一次运行的Pod实例不超过1个。缺点是，如果节点无响应，则副本将永远不会被删除和重新创建。 多副本部署并启用领导者选举（如果容器支持）。对管理员而言这样做的好处是在主副本出现故障时能够更快的恢复，但要占用更多的资源（尤其是内存）。 单个副本部署并启用领导者选举（如果容器支持）。以上两个选项之间的折衷。如果检测到副本失败，则几乎可以立即调度一个新副本。  请注意，某些群集级别的容器，例如 external-provisioner、external-attacher、external-resizer和 external-snapshotter，可能需要存储后端的凭据，因此管理员可以选择在不运行用户容器的专用“基础结构”节点（例如主节点）上运行它们。\n节点对象中的拓扑表示 拓扑信息将使用标签表示。\n要求：\n 必须遵守标签格式。 必须在同一节点上支持不同的驱动程序。 每个键/值对的格式必须与PersistentVolume和StorageClass对象中的格式匹配，如制备和删除部分中所述。  拟议： \u0026quot;com.example.topology/rack\u0026quot;: \u0026quot;rack1\u0026quot; 驱动程序已知的拓扑键列表在CSINodeInfo对象中分别存放。\n理由：\n 与替代方法相比，不需要奇怪的分隔符。更干净的格式。 相同的拓扑键可用于不同的组件（不同的存储插件，网络插件等） 一旦将NodeRestriction移至较新的模型（相关上下文请参见这里），对于新驱动程序中引入的每个新标签前缀，集群管理员必须配置NodeRestrictions以允许驱动程序更新带有前缀的标签。默认情况的集群安装可以为预安装的驱动程序包含某些前缀。与替代方法相比，这种方法不那么方便，后者可以默认使用“csi.kubernetes.io”前缀编辑所有CSI驱动程序，但是集群管理员仍然经常将这些前缀列入白名单（例如\u0026rsquo;cloud.google.com \u0026lsquo;）  注意事项：\n 驱动程序删除/升级/降级后，过时的标签将保持不变。驱动程序很难确定CSI之外的其他组件是否依赖此标签。 在驱动程序安装/升级/降级期间，由于部署依赖于最新的节点信息，因此必须在部署节点之前先停止控制器部署，并且必须在部署控制器之前先部署节点。一个可能的问题是，如果在键保持不变的情况下仅拓扑值发生更改，并且如果未指定AllowedTopologies，则必需的拓扑将同时包含新的和旧的拓扑值，并且对CSI驱动程序的CreateVolume() 调用可能失败。鉴于CSI驱动程序应向后兼容，因此当在控制器更新之前进行节点滚动升级时，这将带来更多问题。如果更改了拓扑建就不存在问题，因为必要的和首选的拓扑生成可以适当地对其进行处理。 在驱动程序安装/升级/降级期间，如果正在运行某个版本的控制器（旧版本或新版本），并且节点部署正在进行滚动升级，并且新版本的CSI驱动程序报告了不同的拓扑信息，则节点中的节点集群可能具有不同版本的拓扑信息。但是，这并不构成问题。如果指定了AllowedTopologies，则与AllowedTopologies中的拓扑信息版本匹配的节点子集将用作置备候选者。如果未指定AllowedTopologies，则单个节点将被用作键的真值源 CSINodeInfo中的拓扑键必须反映节点上当前安装的驱动程序中的拓扑键。如果未安装驱动程序，则集合必须为空。但是，由于kubelet（写者）与外部provisioner（读者）之间可能存在竞争，provisioner必须妥善处理CSINodeInfo不是最新的情况。在当前设计中，provisioner将错误地在无法访问的节点上制备卷。  替代方案：\n \u0026quot;csi.kubernetes.io/topology.example.com_rack\u0026quot;: \u0026quot;rack1\u0026quot;  PersistentVolume对象中的拓扑表示 存在多种将单个拓扑表示为NodeAffinity的方法。例如，假设CreateVolumeResponse包含以下可访问的拓扑：\n- zone: \u0026#34;a\u0026#34; rack: \u0026#34;1\u0026#34; - zone: \u0026#34;b\u0026#34; rack: \u0026#34;1\u0026#34; - zone: \u0026#34;b\u0026#34; rack: \u0026#34;2\u0026#34; 至少有3种方法可以在NodeAffinity中表示（为简单起见，不包括nodeAffinity、required、和 nodeSelectorTerms）：\n形式1-values恰好包含1个元素。\n- matchExpressions: - key: zone operator: In values: - \u0026#34;a\u0026#34; - key: rack operator: In values: - \u0026#34;1\u0026#34; - matchExpressions: - key: zone operator: In values: - \u0026#34;b\u0026#34; - key: rack operator: In values: - \u0026#34;1\u0026#34; - matchExpressions: - key: zone operator: In values: - \u0026#34;b\u0026#34; - key: rack operator: In values: - \u0026#34;2\u0026#34; 形式2-使用rack简化。\n- matchExpressions: - key: zone operator: In values: - \u0026#34;a\u0026#34; - \u0026#34;b\u0026#34; - key: rack operator: In values: - \u0026#34;1\u0026#34; - matchExpressions: - key: zone operator: In values: - \u0026#34;b\u0026#34; - key: rack operator: In values: - \u0026#34;2\u0026#34; 形式3-使用zone简化。\n- matchExpressions: - key: zone operator: In values: - \u0026#34;a\u0026#34; - key: rack operator: In values: - \u0026#34;1\u0026#34; - matchExpressions: - key: zone operator: In values: - \u0026#34;b\u0026#34; - key: rack operator: In values: - \u0026#34;1\u0026#34; - \u0026#34;2\u0026#34; provisioner将始终选择形式1，即所有 values最多具有1个元素。将来的版本中可能选择有效且更简单的形式以简化逻辑，例如形式2和形式3。\n升级和降级注意事项 卸载驱动程序后，存储在节点标签中的拓扑信息将保持不变。推荐的标签格式允许多个源（例如CSI，网络资源等）共享相同的标签键，因此准确确定标签是否仍在使用并非易事。\n为了使用推荐的驱动程序部署机制升级驱动程序，建议用户在部署DaemonSet（节点组件）之前停止StatefulSet（控制器组件），并在StatefulSet之前部署DaemonSet。可能可以进行一些设计改进以消除此约束，但这将在以后的迭代中进行评估。\n示例演练 制备卷  集群管理员创建一个指向CSI驱动程序的external-provisioner的StorageClass，并指定该驱动程序所需的参数。 用户参考新的StorageClass创建一个PersistentVolumeClaim。 持久卷控制器意识到需要动态配置，并在PVC上标注了volume.beta.kubernetes.io/storage-provisioner annotation。 external-provisioner CSI驱动程序会看到带有volume.beta.kubernetes.io/storage-provisioner注释的PersistentVolumeClaim，从而开始动态卷制备：  它反向引用StorageClass以收集用于配置的不透明参数。 它使用来自StorageClass和PersistentVolumeClaim对象的参数对CSI驱动程序容器调用CreateVolume。   一旦成功创建了卷，external-provisioner 就会创建一个PersistentVolume对象来表示新创建的卷，并将其绑定到PersistentVolumeClaim。  删除卷  用户删除绑定​​到CSI卷的PersistentVolumeClaim对象。 CSI驱动程序的external-provisioner看到PersistentVolumeClaim被删除并触发了保留策略： 如果保留策略是delete  external-provisioner通过对CSI卷插件容器发出DeleteVolume调用来触发卷删除。 2. 一旦成功删除该卷，external-provisioner就会删除相应的PersistentVolume对象。   如果保留策略是retain  external-provisioner不会删除PersistentVolume对象。    附加卷  Kubernetes附加/分离控制器作为主节点上的kube-controller-manager二进制文件的一部分运行，发现引用CSI卷插件的Pod已调度到一个节点，因此它调用树内CSI卷插件的attach方法。 树内卷插件在kubernetes API中创建了一个新的VolumeAttachment对象，并等待其状态变更为“completed”或“error”。 external-attacher看到VolumeAttachment对象，并针对CSI卷驱动程序容器触发一个 ControllerPublish调用来实现它（这意味着external-attacher容器通过底层UNIX域套接字向CSI驱动程序容器发出gRPC调用）。 成功完成ControllerPublish 调用后，external-attacher将更新VolumeAttachment 对象的状态，以指示该卷已成功附加。 树内卷插件在Kubernetes API中监听VolumeAttachment\u0026lsquo;对象的状态，看到将Attached字段设置为true指示已连接卷之后，它会更新附加/分离控制器的内部状态以指示卷已经附加。  分离卷  Kubernetes附加/分离控制器作为主节点上的kube-controller-manager二进制文件的一部分运行，它看到引用附加的CSI卷插件的Pod已终止或删除，因此它调用树中CSI卷插件的分离方法。 树内卷插件将删除相应的VolumeAttachment对象。 external-attacher看到在VolumeAttachment对象上设置的deletionTimestamp后，会针对CSI卷驱动器容器触发ControllerUnpublish以将其分离。 成功完成对ControllerUnpublish的调用后，外部连接器将从VolumeAttachment对象中删除终结器，以指示分离操作成功完成，从而允许删除VolumeAttachment对象。 树内卷插件等待VolumeAttachment对象，观察他是否被删除，并且假设卷已经成功分离，这是他将会更新附加/分离控制器的内部状态以指示卷已经分离。  挂载卷  kubelet的卷管理器组件会注意到已将一个新的卷（已引用CSI卷）调度到该节点，因此它将调用树内CSI卷插件的WaitForAttach方法。 树内卷插件的WaitForAttach方法监监听kubernetes API中VolumeAttachment对象的Attached字段直到他变为true，然后成功返回。 然后，Kubelet调用树内CSI卷插件的MountDevice方法，该方法无操作，之后立即返回。 最后，kubelet调用树内CSI卷插件的挂载（设置）方法，该方法使树内卷插件通过已注册的unix域套接字向本地CSI驱动程序发出NodePublishVolume调用。 成功完成NodePublishVolume调用后，指定的路径将被挂载到pod容器中。  卸载卷  kubelet的卷管理器组件会注意到引用已挂载的CSI卷的pod已被已删除或终止，因此它将调用树中CSI卷插件的UnmountDevice方法（该方法是无操作的）并立即返回。 然后kubelet调用树内CSI卷插件的卸载（teardown）方法，这将导致树内卷插件通过已注册的unix域套接字向本地CSI驱动程序发出NodeUnpublishVolume调用。如果此调用由于任何原因失败，则kubelet会定期重试该调用。 成功完成NodeUnpublishVolume调用后，将从pod容器中卸载指定的路径。  CSI凭据 CSI允许在CreateVolume/DeleteVolume，ControllerPublishVolume/ControllerUnpublishVolume，NodeStageVolume/NodeUnstageVolume和NodePublishVolume/NodeUnpublishVolume操作中指定凭据。\nKubernetes将使集群管理员和在集群上部署工作负载的用户能够通过引用Kubernetes secret对象来指定这些凭据。Kubernetes（核心组件或辅助容器）将获取secret并将其传递给CSI卷插件。\n如果一个secret对象包含多个值，所有值都会被传递。\nCSI凭证secret编码 CSI接受以上指定的所有操作的凭据，作为字符串到字符串的映射（例如，map\u0026lt;string, string\u0026gt; controller_create_credentials）。\n但是Kubernetes将secret定义为字符串到字节数组的映射（例如，Data map[string][]byte）。它还允许通过快捷字段StringData以字符串形式指定文本secret数据，该字段是字符串到字符串的映射。\n因此，在将secret数据传递给CSI之前，Kubernetes（核心组件或辅助容器）会将secret数据从字节转换为字符串（Kubernetes未指定字符编码，但是Kubernetes在内部使用golang将字符串从字符串转换为字节，反之亦然）反之亦然（假设使用UTF-8字符集）。\n尽管CSI仅接受字符串数据，但是插件可以在其文档中指示特定secret包含二进制数据，并指定要使用的二进制文本编码（base64，quoted-printable等）来编码二进制数据并允许它以字符串形式传递。创建secret并确保其内容符合插件期望的内容并以插件期望的格式进行编码是实体（集群管理员，用户等）的责任。\nCreateVolume/DeleteVolume凭证 CSI CreateVolume/DeleteVolume调用负责创建和删除卷。 这些调用由CSI external-provisioner执行。 这些调用的凭据将在Kubernetes的StorageClass对象中指定。\nkind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: fast-storage provisioner: com.example.team.csi-driver parameters: type: pd-ssd csiProvisionerSecretName: mysecret csiProvisionerSecretNamespace: mynamespaace CSI external-provisioner将保存参数键csiProvisionerSecretName和csiProvisionerSecretNamespace。如果指定，则CSI Provisioner将在Kubernetes命名空间csiProvisionerSecretNamespace中获取secret csiProvisionerSecretName并将其传递给：\n CSI CreateVolumeRequest中通过controller_create_credentials字段。 TCSI DeleteVolumeRequest中通过controller_delete_credentials字段。  有关如何将secret映射到CSI凭证的详细信息，请参见上面的“CSI凭证secret编码”部分。\n这基于假设：由于StorageClass是一个non-namespaced的字段，因此只有受信任的用户（例如集群管理员）才能创建StorageClass，从而指定要获取的secret。\n唯一需要访问此机密的Kubernetes组件是CSI external-provisioner，它将获取此secret。可以将external-provisioner的权限限制为指定的（特定于external-provisioner的）名称空间，以防止损坏的的供应商获得对其他secret的访问权限。\nControllerPublishVolume/ControllerUnpublishVolume凭证 CSI ControllerPublishVolume/ControllerUnpublishVolume调用负责附加和分离卷。 这些调用由CSI external-attacher执行。 这些调用的凭据将在Kubernetes的CSIPersistentVolumeSource对象中指定。\ntype CSIPersistentVolumeSource struct { // ControllerPublishSecretRef is a reference to the secret object containing  // sensitive information to pass to the CSI driver to complete the CSI  // ControllerPublishVolume and ControllerUnpublishVolume calls.  // This secret will be fetched by the external-attacher.  // This field is optional, and may be empty if no secret is required. If the  // secret object contains more than one secret, all secrets are passed.  // ControllerPublishSecretRef是对包含敏感信息的secret对象的引用，该敏感信息将传递给CSI驱  // 动程序以完成CSI ControllerPublishVolume和ControllerUnpublishVolume调用。  // 这个secret将由external-attacher获取。  // 该字段可选，如果不需要任何secret，则可以为空。如果secret对象包含多个秘密信息，则将传递所有秘密。  // +optional  ControllerPublishSecretRef *SecretReference } 如果指定，则CSI外部附加程序将获取ControllerPublishSecretRef引用的Kubernetes secret并将其传递给：\n 在CSI ControllerPublishVolume中通过controller_publish_credentials字段传递。 再CSI ControllerUnpublishVolume中通过controller_unpublish_credentials字段传递。  有关如何将secret映射到CSI凭证的详细信息，请参见上面的“CSI凭证secret编码”部分。\n这基于假设：由于PersistentVolume是一个non-namespaced的字段，因此只有受信任的用户（例如集群管理员）才能创建CSIPersistentVolumeSource，从而指定要获取的secret。\nThe only Kubernetes component that needs access to this secret is the CSI external-attacher, which would fetch this secret. The permissions for the external-attacher may be limited to the specified (external-attacher specific) namespace to prevent a compromised attacher from gaining access to other secrets.\nNodeStageVolume/NodeUnstageVolume Credentials The CSI NodeStageVolume/NodeUnstageVolume calls are responsible for mounting (setup) and unmounting (teardown) volumes. 这些调用由Kubernetes节点代理（kubelet）执行。 这些调用的凭据将在Kubernetes的CSIPersistentVolumeSource对象中指定。\ntype CSIPersistentVolumeSource struct { // NodeStageSecretRef is a reference to the secret object containing sensitive  // information to pass to the CSI driver to complete the CSI NodeStageVolume  // and NodeStageVolume and NodeUnstageVolume calls.  // This secret will be fetched by the kubelet.  // This field is optional, and may be empty if no secret is required. If the  // NodeStageSecretRef是对包含敏感信息的secret对象的引用，该敏感信息将传递给CSI驱动程序以完成CSI // NodeStageVolume、NodeStageVolume和NodeUnstageVolume调用。这个secret将由kubelet获取。  // 该字段可选，如果不需要任何secret，则可以为空。如果secret对象包含多个秘密信息，则将传递所有秘密。  // +optional  NodeStageSecretRef *SecretReference } If specified, the kubelet will fetch the Kubernetes secret referenced by NodeStageSecretRef and pass it to:\n The CSI NodeStageVolume in the node_stage_credentials field. The CSI NodeUnstageVolume in the node_unstage_credentials field.  有关如何将secret映射到CSI凭证的详细信息，请参见上面的“CSI凭证secret编码”部分。\n这基于假设：由于PersistentVolume是一个non-namespaced的字段，因此只有受信任的用户（例如集群管理员）才能创建CSIPersistentVolumeSource，从而指定要获取的secret。\n需要访问此secret的唯一Kubernetes组件是kubelet，它将获取此secret。可以将kubelet的权限限制为指定的（特定于kubelet的）名称空间，以防止损坏的附加程序获得对其他secret的访问权限。\n必须更新Kubernetes API服务器的节点授权者，以允许kubelet访问CSIPersistentVolumeSource.NodeStageSecretRef引用的secret。\nNodePublishVolume/NodeUnpublishVolume凭证 CSI NodePublishVolume/NodeUnpublishVolume调用负责挂载（设置）和卸载（拆卸）卷。 这些调用由Kubernetes节点代理（kubelet）执行。 这些调用的凭据将在Kubernetes的CSIPersistentVolumeSource对象中指定。\ntype CSIPersistentVolumeSource struct { // NodePublishSecretRef is a reference to the secret object containing  // sensitive information to pass to the CSI driver to complete the CSI  // NodePublishVolume and NodeUnpublishVolume calls.  // This secret will be fetched by the kubelet.  // This field is optional, and may be empty if no secret is required. If the  // secret object contains more than one secret, all secrets are passed.  // NodePublishSecretRef是对包含敏感信息的secret对象的引用，该敏感信息将传递给CSI驱动程序以完成CSI // NodePublishVolume和NodeUnpublishVolume调用。这个secret将由kubelet获取。  // 该字段可选，如果不需要任何secret，则可以为空。如果secret对象包含多个秘密信息，则将传递所有秘密。  // +optional  NodePublishSecretRef *SecretReference } 如果指定，则kubelet将获取由NodePublishSecretRef引用的Kubernetes secret，并将其传递给：\n CSI NodePublishVolume中的node_publish_credentials字段。 CSI NodeUnpublishVolume中的node_unpublish_credentials字段。  有关如何将secret映射到CSI凭证的详细信息，请参见上面的“CSI凭证secret编码”部分。\n这基于假设：由于PersistentVolume是一个non-namespaced的字段，因此只有受信任的用户（例如集群管理员）才能创建CSIPersistentVolumeSource，从而指定要获取的secret。\n需要访问此secret的唯一Kubernetes组件是kubelet，它将获取此secret。可以将kubelet的权限限制为指定的（特定于kubelet的）名称空间，以防止损坏的附加程序获得对其他secret的访问权限。\n必须更新Kubernetes API服务器的节点授权者，以允许kubelet访问CSIPersistentVolumeSource.NodePublishSecretRef引用的secret。\n考虑的替代方案 扩展PersistentVolume对象 除了创建新的VolumeAttachment对象外，我们考虑的另一种选择是扩展现有的PersistentVolume对象。\nPersistentVolumeSpec将扩展为包括：\n 将卷附加到的节点列表（最初为空）。  PersistentVolumeStatus将扩展为包括：\n 卷已成功附加到的节点列表。  我们没有用这种方法，因为由对象的创建/删除触发的附加/分离更容易管理（对于外部附加器和Kubernetes）并且更健壮（无需担心的极端情况）。\n"
            }
        
    ,
        
            {
                "ref": "https://xiaosuiba.github.io/2021/05/10/2021-05-10-how-does-tkestack-localidentity-make-use-of-generatename/",
                "title": "TKEStack中LocalIdentity如何利用generateName机制生成Name",
                "section": "post",
                "date" : "2021.05.10",
                "body": "本文从发现TKEStack自动生成localidentity名称机制出发，稍微探索了kubernetes generateName的作用机理。\n日前在研究TKEStack用户机制中，发现创建LocalIdentiy时，并没有传入metadata.name，而最终系统会生成一个随机值，类似：\nusr-2t455l2z usr-dsa54sdf usr-xxxxxxxx 这显然是一个随机值。由于我们希望自定义这个值，所以进行了一番测试，结果发现：\n 不设置metadata.name时，生成随机值 设置metadata.name之后，使用设置值  显然这个值是可以直接设置的。但是我对这个随机值的生成产生了兴趣，带着问题翻看了下源码，在pkg\\auth\\registry\\localidentity\\strategy.go中，找到如下片段：\nfunc (Strategy) PrepareForCreate(ctx context.Context, obj runtime.Object) { localIdentity, _ := obj.(*auth.LocalIdentity) _, tenantID := authentication.UsernameAndTenantID(ctx) if len(tenantID) != 0 { localIdentity.Spec.TenantID = tenantID } if localIdentity.Name == \u0026#34;\u0026#34; \u0026amp;\u0026amp; localIdentity.GenerateName == \u0026#34;\u0026#34; { localIdentity.GenerateName = \u0026#34;usr-\u0026#34; } localIdentity.Spec.Finalizers = []auth.FinalizerName{ auth.LocalIdentityFinalize, } } 这是localIdentity reststorage的创建预处理函数，可以看到其中对localIdentity.Name做了检查，但是这里却没有直接生成随机值，而是对GenerateName进行了一个赋值。那最终的随机值Name是怎么生成了呢？\n看到这个usr-，其实已经猜到了一大半，猜想这里设置的是一个前缀，而随机值部分应该是由K8s自身的机制实现的。官网相关文档解释如下：\n Generated values Some values of an object are typically generated before the object is persisted. It is important not to rely upon the values of these fields set by a dry-run request, since these values will likely be different in dry-run mode from when the real request is made. Some of these fields are:\n name: if generateName is set, name will have a unique random name creationTimestamp/deletionTimestamp: records the time of creation/deletion UID: uniquely identifies the object and is randomly generated (non-deterministic) resourceVersion: tracks the persisted version of the object Any field set by a mutating admission controller For the Service resource: Ports or IPs that kube-apiserver assigns to v1.Service objects   这里说得比较含糊，而stackoverflow中有个问题说得很明确：\n You can replace name with generateName, which adds a random suffix.\n 以后在需要随机值的场合，可以不用自己动手了，直接依赖系统机制即可。\n"
            }
        
    ,
        
            {
                "ref": "https://xiaosuiba.github.io/2021/04/13/2021-04-13-tkestack-monitor-controller-issue/",
                "title": "TKEStack v1.6.0 1.19版本中pod告警策略失效问题分析",
                "section": "post",
                "date" : "2021.04.13",
                "body": "本文从探索了TKEStack 1.6中部分告警失效的原因，并提出修复手段。\n日前在使用TKEStack v1.6.0的时候，发现针对工作负载的（也就是pod状态、重启次数）告警失效。我们进行了一定的分析，最终找到原因和Kubernetes 1.19以后版本中一个kubelet metrics名称修改有关。\nTKEStack v1.6.0针对pod的告警使用了k8s_pod_status_ready, k8s_pod_restart_total两个metrics作为判定参数。继续查看这两个metrics的定义，发现他们都与一个名为__pod_info2的metrics相关，其定义如下：\n- record: __pod_info2 expr: label_replace(label_replace(__pod_info1{workload_kind=\u0026#34;ReplicaSet\u0026#34;} * on (workload_name,namespace) group_left(owner_name, owner_kind) label_replace(kube_replicaset_owner,\u0026#34;workload_name\u0026#34;,\u0026#34;$1\u0026#34;,\u0026#34;replicaset\u0026#34;,\u0026#34;(.*)\u0026#34;),\u0026#34;workload_name\u0026#34;,\u0026#34;$1\u0026#34;,\u0026#34;owner_name\u0026#34;,\u0026#34;(.*)\u0026#34;),\u0026#34;workload_kind\u0026#34;,\u0026#34;$1\u0026#34;,\u0026#34;owner_kind\u0026#34;,\u0026#34;(.*)\u0026#34;) or on(pod_name,namesapce) __pod_info1{workload_kind != \u0026#34;ReplicaSet\u0026#34;} __pod_info2又与__pod_info1相关，其定义如下：\n- record: __pod_info1 expr: kube_pod_info* on(node) group_left(node_role) kube_node_labels 由__pod_info1，关联到kube_node_labels。到这里时，我们发现在v1.19版本中，kube_node_labels已经没有数据，看来问题就出在这里。查看其定义如下：\n- record: kube_node_labels expr: kubelet_running_pod_count*0 + 1 定义很简单，只与kubelet_running_pod_count相关，而这个值在prometheus中也未能查询到，看来问题发生在这里。\n简单的google之后，我们迅速发现了问题的原因：kubernetes在v1.19版本中进行了两个kubelet metrics的修改(kubernetes/kubernetes#92407)：\nkubelet: following metrics have been renamed: kubelet_running_container_count --\u0026gt; kubelet_running_containers kubelet_running_pod_count --\u0026gt; kubelet_running_pods 该修改造成了kube_node_labels的失效，从而导致许多tke自定义的metrics失效。\n解决方案有两种：\n 按选择版本配置不同的prometheus 使用正则表达式匹配所有版本的metrics  毫无疑问第二种方式更加简单。相关代码位于：pkg/monitor/controller/prometheus/yamls.go 测试之后问题解决。相关issue和pr已经提交到TKEStack。\n"
            }
        
    ,
        
            {
                "ref": "https://xiaosuiba.github.io/2021/04/06/2021-04-06-tkestack-auth-api-issue/",
                "title": "TKEStack v1.6.0 global集群中serviceaccount总是默认拥有所有权限",
                "section": "post",
                "date" : "2021.04.06",
                "body": "本文从探索了TKEStack 1.6中global集群任何serviceaccount均具有cluster-admin权限原因。\nTKEStack v1.6.0已经发布了，没有包含重大更新，但是在使用过程中，我们发现了一个很神奇的现象：global集群中任何serviceaccount都能访问所有的集群资源。这点可以直接使用kubectl auth can-i得到验证：\n在1.5.0集群中执行：\n$ kubectl auth can-i create pod --as=system:serviceaccount:tke:fake no 而在1.6.0中执行：\n$ kubectl auth can-i create pod --as=system:serviceaccount:tke:fake yes 我们甚至都还没有创建过fake这个serviceaccount。根据现象，首先怀疑是kube-apiserver的authz配置发生了改变，查看kube-apiserver的配置，果然:\n--authorization-mode=Node,RBAC,Webhook 1.5.0中则只有Node,RBAC，看来问题就出在Webhook中。关于Webhook的说明，官方文档给出了解释。继续查看kubernetes的Webhook配置，配置文件为--authorization-webhook-config-file=/etc/kubernetes/tke-authz-webhook.yaml，文件内容如下：\ntke-authz-webhook.yaml: |apiVersion: v1 kind: Config clusters: - name: tke cluster: certificate-authority: /app/certs/ca.crt server: http://{vip}:31138/auth/authz users: - name: admin-cert user: client-certificate: /app/certs/admin.crt client-key: /app/certs/admin.key current-context: tke contexts: - context: cluster: tke user: admin-cert name: tke webhook代理地址为http://{vip}:31138/auth/authz，也即是tke-auth-api的nodeport。\n继续查看tke-auth-api中认证相关配置： /cmd/tke-auth-api/app/app.go\ncfg, err := config.CreateConfigFromOptions(basename, opts) tke-auth-api/app/config/config.go\naggregateAuthz, err := aggregation.NewAuthorizer(authClient, opts.Authorization, opts.Auth, enforcer, opts.Authentication.PrivilegedUsername) /opt/project/tke/pkg/auth/authorization/aggregation/aggregation.go\n// NewAuthorizer creates a authorizer for subject access review and returns it. func NewAuthorizer(authClient authinternalclient.AuthInterface, authorizationOpts *options.AuthorizationOptions, authOpts *options.AuthOptions, enforcer *casbin.SyncedEnforcer, privilegedUsername string) (authorizer.Authorizer, error) { var ( authorizers []authorizer.Authorizer ) if len(authorizationOpts.WebhookConfigFile) != 0 { webhookAuthorizer, err := webhook.New(authorizationOpts.WebhookConfigFile, authorizationOpts.WebhookVersion, authorizationOpts.WebhookCacheAuthorizedTTL, authorizationOpts.WebhookCacheUnauthorizedTTL, nil) if err != nil { return nil, err } authorizers = append(authorizers, webhookAuthorizer) } if len(authorizationOpts.PolicyFile) != 0 { abacAuthorizer, err := abac.NewABACAuthorizer(authorizationOpts.PolicyFile) if err != nil { return nil, err } authorizers = append(authorizers, abacAuthorizer) } authorizers = append(authorizers, local.NewAuthorizer(authClient, enforcer, privilegedUsername)) return union.New(authorizers...), nil } 可以看到最终的authrizer配置由webhook（如果有）、abac（如果有）和local组成。auth-api的配置文件：\ntke-auth-api.toml: | ........ [authorization] policy_file=\u0026#34;/app/conf/abac-policy.json\u0026#34; 继续查看abac-policy.json\n{\u0026#34;apiVersion\u0026#34;:\u0026#34;abac.authorization.kubernetes.io/v1beta1\u0026#34;,\u0026#34;kind\u0026#34;:\u0026#34;Policy\u0026#34;,\u0026#34;spec\u0026#34;:{\u0026#34;user\u0026#34;:\u0026#34;system:*\u0026#34;,\u0026#34;namespace\u0026#34;:\u0026#34;*\u0026#34;, \u0026#34;resource\u0026#34;:\u0026#34;*\u0026#34;,\u0026#34;apiGroup\u0026#34;:\u0026#34;*\u0026#34;, \u0026#34;group\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;nonResourcePath\u0026#34;:\u0026#34;*\u0026#34;}} 该文件将配置任意system:*配置拥有任意namespace下的所有资源。\n至此，问题原因已经找到了。但问什么TKEStack中如此配置ABAC？这将导致一个明显的漏洞出现。带着问题，我们继续查看github上的修改提交记录：\n 1155 但是没有找到任何相关说明为何要如此修改，我将对此issue进行一个comment，希望作者能有相关解释。 "
            }
        
    ,
        
            {
                "ref": "https://xiaosuiba.github.io/2021/03/31/2021-03-31-tkestack-registry-dns-issue/",
                "title": "TKEStack组件不能访问registry域名问题",
                "section": "post",
                "date" : "2021.03.31",
                "body": "本文从探索了TKEStack 1.5版本中TKEStack组件不能访问registry域名的原因，并给出解决办法。\nTKEStack需要配置registry域名，默认为default.registry.tke.com，application组件将使用该域名解析到tke-registry-api服务（或者直接解析到gateway也可以）。\n在v1.5版本中，Coredns默认没有配置对该域名的处理，导致用户只能通过Coredns的forward插件，使用外部的DNS做处理；或者修改Coredns配置文件。\n很高兴看到在v1.6.0版本中，该问题得到了解决，Corefile中添加了如下配置：\n ……………… rewrite name default.registry.tke.com tke-registry-api.tke.svc.cluster.local 官方对于rewrite插件的解释如下：\n Rewrites are invisible to the client. There are simple rewrites (fast) and complex rewrites (slower), but they’re powerful enough to accommodate most dynamic back-end applications.\n 也即将default.registry.tke.comrewrite到tke-registry-api.tke.svc.cluster.local，也即tke-registry-api地址。\n相关pr: feat(registry): add registry\u0026rsquo;s domain names to coredns by tenant\n"
            }
        
    ,
        
            {
                "ref": "https://xiaosuiba.github.io/2021/03/29/2021-03-29-why-only-new-in-rest-storage-interface-copy/",
                "title": "Why rest.Storage interface contains only one method",
                "section": "post",
                "date" : "2021.03.29",
                "body": "rest.Storage interface contains only one method. This page show how could it utilize golang reflection to do the job.\nWhy rest.Storage interface have only one method While reading Kubernetes source code, I found one question really bothered me for several days. The common interface of rest storage is defined in /vendor/k8s.io/apiserver/pkg/registry/rest/rest.go:\ntype Storage interface { // New returns an empty object that can be used with Create and Update after request data has been put into it. \t// This object must be a pointer type for use with Codec.DecodeInto([]byte, runtime.Object) \tNew() runtime.Object } You can see there\u0026rsquo;s only one method New defined here. So how could the other actions, like Get or List etc., be completed?\nAfter several day\u0026rsquo;s search on the internet, I finally found the some code in /vendor/k8s.io/apiserver/pkg/endpoints/installer.go:\nfunc (a *APIInstaller) registerResourceHandlers(path string, storage rest.Storage, ws *restful.WebService) (*metav1.APIResource, *storageversion.ResourceInfo, error) { ..... // what verbs are supported by the storage, used to know what verbs we support per path \tcreater, isCreater := storage.(rest.Creater) namedCreater, isNamedCreater := storage.(rest.NamedCreater) lister, isLister := storage.(rest.Lister) getter, isGetter := storage.(rest.Getter) getterWithOptions, isGetterWithOptions := storage.(rest.GetterWithOptions) gracefulDeleter, isGracefulDeleter := storage.(rest.GracefulDeleter) collectionDeleter, isCollectionDeleter := storage.(rest.CollectionDeleter) updater, isUpdater := storage.(rest.Updater) patcher, isPatcher := storage.(rest.Patcher) watcher, isWatcher := storage.(rest.Watcher) connecter, isConnecter := storage.(rest.Connecter) storageMeta, isMetadata := storage.(rest.StorageMetadata) storageVersionProvider, isStorageVersionProvider := storage.(rest.StorageVersionProvider) see here, registerResourceHandlers utilizes golang type conversion to check the storage\u0026rsquo;s abilities. So the New method is only a sign of rest.storage interface. The real abilities could be defined in resource storage struct as needed. The methods defined in each specific resource storage type represents its ability in the same time.\n"
            }
        
    ,
        
            {
                "ref": "https://xiaosuiba.github.io/2021/03/26/2021-03-26-kubernetes-sample-apiserver/",
                "title": "Kubernetes sample-apiserver 代码阅读",
                "section": "post",
                "date" : "2021.03.26",
                "body": "启动过程 main.go:\nfunc main() { logs.InitLogs() defer logs.FlushLogs() stopCh := genericapiserver.SetupSignalHandler() options := server.NewWardleServerOptions(os.Stdout, os.Stderr) cmd := server.NewCommandStartWardleServer(options, stopCh) cmd.Flags().AddGoFlagSet(flag.CommandLine) if err := cmd.Execute(); err != nil { klog.Fatal(err) } } options 调用 server.NewWardleServerOption 构建了一个 WardleServerOptions 配置对象\ntype WardleServerOptions struct { RecommendedOptions *genericoptions.RecommendedOptions SharedInformerFactory informers.SharedInformerFactory StdOut io.Writer StdErr io.Writer } RecommendedOptions 的解释为：\n // RecommendedOptions contains the recommended options for running an API server.\n// If you add something to this list, it should be in a logical grouping.\n// Each of them can be nil to leave the feature unconfigured on ApplyTo.\n server.NewCommandStartWardleServer 中构建了一个 cobra.Command，代码很简短：\nfunc NewCommandStartWardleServer(defaults *WardleServerOptions, stopCh \u0026lt;-chan struct{}) *cobra.Command { o := *defaults cmd := \u0026amp;cobra.Command{ Short: \u0026#34;Launch a wardle API server\u0026#34;, Long: \u0026#34;Launch a wardle API server\u0026#34;, RunE: func(c *cobra.Command, args []string) error { if err := o.Complete(); err != nil { return err } if err := o.Validate(args); err != nil { return err } if err := o.RunWardleServer(stopCh); err != nil { return err } return nil }, } flags := cmd.Flags() o.RecommendedOptions.AddFlags(flags) utilfeature.DefaultMutableFeatureGate.AddFlag(flags) return cmd } 可以看到主要工作包括两部分，一个是使用flag填充o.RecommendedOption，这部分在调用时即完成；第而部分是配置cmd的启动RunE，主要有三个步骤：\n o.Complete(): 完成一些额外配置，如admission plugins o.Validate(args)：对配置进行校验，这里直接使用了默认校验（sample-apiserver没有额外的配置项） o.RunWardleServer(stopCh)：启动服务  下面重点看下o.RunWardleServer，这是启动服务的核心部分：\n// RunWardleServer starts a new WardleServer given WardleServerOptions func (o WardleServerOptions) RunWardleServer(stopCh \u0026lt;-chan struct{}) error { //从RecommendedOption产生最终config \tconfig, err := o.Config() if err != nil { return err } //对config进行补完并使用配置产生server \tserver, err := config.Complete().New() if err != nil { return err } server.GenericAPIServer.AddPostStartHookOrDie(\u0026#34;start-sample-server-informers\u0026#34;, func(context genericapiserver.PostStartHookContext) error { config.GenericConfig.SharedInformerFactory.Start(context.StopCh) o.SharedInformerFactory.Start(context.StopCh) return nil }) //启动服务 \treturn server.GenericAPIServer.PrepareRun().Run(stopCh) } o.Config()：\n// Config returns config for the api server given WardleServerOptions func (o *WardleServerOptions) Config() (*apiserver.Config, error) { …… if err := o.RecommendedOptions.ApplyTo(serverConfig); err != nil { return nil, err } config := \u0026amp;apiserver.Config{ GenericConfig: serverConfig, ExtraConfig: apiserver.ExtraConfig{}, } return config, nil } 从这里可以发现通常的apiserver.Config构建方式是通过RecommendedOptions与cobra.command协作构建出RecommendedOptions，然后使用RecommendedOptions.ApplyTo方法将配置填充到apiserver.Config中，用于启动最终的apiserver。\nserver, err := config.Complete().New()产生了最终的WardleServer：\nfunc (c completedConfig) New() (*WardleServer, error) { genericServer, err := c.GenericConfig.New(\u0026#34;sample-apiserver\u0026#34;, genericapiserver.NewEmptyDelegate()) if err != nil { return nil, err } s := \u0026amp;WardleServer{ GenericAPIServer: genericServer, } apiGroupInfo := genericapiserver.NewDefaultAPIGroupInfo(wardle.GroupName, Scheme, metav1.ParameterCodec, Codecs) v1alpha1storage := map[string]rest.Storage{} v1alpha1storage[\u0026#34;flunders\u0026#34;] = wardleregistry.RESTInPeace(flunderstorage.NewREST(Scheme, c.GenericConfig.RESTOptionsGetter)) v1alpha1storage[\u0026#34;fischers\u0026#34;] = wardleregistry.RESTInPeace(fischerstorage.NewREST(Scheme, c.GenericConfig.RESTOptionsGetter)) apiGroupInfo.VersionedResourcesStorageMap[\u0026#34;v1alpha1\u0026#34;] = v1alpha1storage v1beta1storage := map[string]rest.Storage{} v1beta1storage[\u0026#34;flunders\u0026#34;] = wardleregistry.RESTInPeace(flunderstorage.NewREST(Scheme, c.GenericConfig.RESTOptionsGetter)) apiGroupInfo.VersionedResourcesStorageMap[\u0026#34;v1beta1\u0026#34;] = v1beta1storage if err := s.GenericAPIServer.InstallAPIGroup(\u0026amp;apiGroupInfo); err != nil { return nil, err } return s, nil } New做了两个重要工作：\n 使用配置构建了genericServer 为apiGroup配置了restStorage  我们挑选flunderstorage.NewREST来看看如何构建一个默认的RESTStorage:\n// NewREST returns a RESTStorage object that will work against API services. func NewREST(scheme *runtime.Scheme, optsGetter generic.RESTOptionsGetter) (*registry.REST, error) { strategy := NewStrategy(scheme) store := \u0026amp;genericregistry.Store{ NewFunc: func() runtime.Object { return \u0026amp;wardle.Flunder{} }, NewListFunc: func() runtime.Object { return \u0026amp;wardle.FlunderList{} }, PredicateFunc: MatchFlunder, DefaultQualifiedResource: wardle.Resource(\u0026#34;flunders\u0026#34;), CreateStrategy: strategy, UpdateStrategy: strategy, DeleteStrategy: strategy, // TODO: define table converter that exposes more than name/creation timestamp \tTableConvertor: rest.NewDefaultTableConvertor(wardle.Resource(\u0026#34;flunders\u0026#34;)), } options := \u0026amp;generic.StoreOptions{RESTOptions: optsGetter, AttrFunc: GetAttrs} if err := store.CompleteWithOptions(options); err != nil { return nil, err } return \u0026amp;registry.REST{store}, nil } 这里直接使用genericregistry.Store构建了RESTStorage，按照RESTStorage-\u0026gt;RegistryStore-\u0026gt;Storage.Interface的流程，我们再深入一下genericregistry.Store，可以看到，在函数中我们并没有配置任何的Storage.Interface（genericregistry.Store中的Storage属性），在store.CompleteWithOptions(options)中：\nif e.Storage.Storage == nil { e.Storage.Codec = opts.StorageConfig.Codec var err error e.Storage.Storage, e.DestroyFunc, err = opts.Decorator( opts.StorageConfig, prefix, keyFunc, e.NewFunc, e.NewListFunc, attrFunc, options.TriggerFunc, options.Indexers, ) 当Storage为nil时，使用Decorator模式构建了一个e.Storage.Storage。\n此外，在apiserver.go中还使用init函数，完成了schema的注册工作：\nfunc init() { install.Install(Scheme) // we need to add the options to empty v1 \t// TODO fix the server code to avoid this \tmetav1.AddToGroupVersion(Scheme, schema.GroupVersion{Version: \u0026#34;v1\u0026#34;}) // TODO: keep the generic API server from wanting this \tunversioned := schema.GroupVersion{Group: \u0026#34;\u0026#34;, Version: \u0026#34;v1\u0026#34;} Scheme.AddUnversionedTypes(unversioned, \u0026amp;metav1.Status{}, \u0026amp;metav1.APIVersions{}, \u0026amp;metav1.APIGroupList{}, \u0026amp;metav1.APIGroup{}, \u0026amp;metav1.APIResourceList{}, ) } 到此为止，apiserver的配置已经完成，接下来我们看看服务的启动: server.GenericAPIServer.PrepareRun().Run(stopCh)\n// PrepareRun does post API installation setup steps. It calls recursively the same function of the delegates. func (s *GenericAPIServer) PrepareRun() preparedGenericAPIServer { s.delegationTarget.PrepareRun() if s.openAPIConfig != nil { s.OpenAPIVersionedService, s.StaticOpenAPISpec = routes.OpenAPI{ Config: s.openAPIConfig, }.Install(s.Handler.GoRestfulContainer, s.Handler.NonGoRestfulMux) } s.installHealthz() s.installLivez() err := s.addReadyzShutdownCheck(s.readinessStopCh) if err != nil { klog.Errorf(\u0026#34;Failed to install readyz shutdown check %s\u0026#34;, err) } s.installReadyz() // Register audit backend preShutdownHook. \tif s.AuditBackend != nil { err := s.AddPreShutdownHook(\u0026#34;audit-backend\u0026#34;, func() error { s.AuditBackend.Shutdown() return nil }) if err != nil { klog.Errorf(\u0026#34;Failed to add pre-shutdown hook for audit-backend %s\u0026#34;, err) } } return preparedGenericAPIServer{s} } 可以看到主要工作就是配置openAPI handler，安装Healthz、Livez、Readyz端点，接下来运行Run：\n// Run spawns the secure http server. It only returns if stopCh is closed // or the secure port cannot be listened on initially. func (s preparedGenericAPIServer) Run(stopCh \u0026lt;-chan struct{}) error { ............... // close socket after delayed stopCh \tstoppedCh, err := s.NonBlockingRun(delayedStopCh) ............... } func (s preparedGenericAPIServer) NonBlockingRun(stopCh \u0026lt;-chan struct{}) (\u0026lt;-chan struct{}, error) { ................ // Use an internal stop channel to allow cleanup of the listeners on error. \tinternalStopCh := make(chan struct{}) var stoppedCh \u0026lt;-chan struct{} if s.SecureServingInfo != nil \u0026amp;\u0026amp; s.Handler != nil { var err error stoppedCh, err = s.SecureServingInfo.Serve(s.Handler, s.ShutdownTimeout, internalStopCh) if err != nil { close(internalStopCh) close(auditStopCh) return nil, err } } ................ } 至此，APIServer启动完成。\n"
            }
        
    ,
        
            {
                "ref": "https://xiaosuiba.github.io/2019/02/02/2020-02-02-happy-spring-festival/",
                "title": "Happy Spring Festival",
                "section": "post",
                "date" : "2019.02.02",
                "body": "Happy Spring Festival 赶在春节前把博客搬迁成 hugo 了 :) 祝大家春节快乐，来年大吉，Happy Spring Festival~\n"
            }
        
    
]